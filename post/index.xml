<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on fly away, chase dream</title>
    <link>/post/</link>
    <description>Recent content in Posts on fly away, chase dream</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 28 Apr 2020 23:37:42 +0800</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>YOLOv4: Optimal Speed and Accuracy of Object Detection论文解读</title>
      <link>/post/yolov4/</link>
      <pubDate>Tue, 28 Apr 2020 23:37:42 +0800</pubDate>
      
      <guid>/post/yolov4/</guid>
      <description>最近目标检测又出了yolo-v4，作为一个做目标检测的不可不膜拜膜拜。首先由于约瑟夫大神已经退出CV，yolo-v4 的一作是DarkNet的</description>
    </item>
    
    <item>
      <title>目标检测IOU评价指标汇总: GIOU, DIOU, CIOU</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</link>
      <pubDate>Fri, 24 Apr 2020 23:18:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</guid>
      <description>在目标检测中, IOU 可以被用来评估预测框的性能，IOU越大预测框越准。IOU可表示两个框的距离，IOU越大距离越小. 对于目标检测坐标损失虽然一般</description>
    </item>
    
    <item>
      <title>谷歌最新目标检测论文: EfficientDet</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</link>
      <pubDate>Sat, 18 Apr 2020 11:55:48 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</guid>
      <description>最近谷歌放出了 EfficientDet 论文与代码, 在COCO上取得了最好的MAP, 本文对 efficientDet 做个简要的总结, 同时对efficientNet也做个回顾. Efficie</description>
    </item>
    
    <item>
      <title>基于内容的图像检索: pytorch</title>
      <link>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</link>
      <pubDate>Fri, 17 Apr 2020 16:33:21 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</guid>
      <description>看了Jason Brownlee博士的Keras CBIR demo, 自己也动手用pytorch写一个. CBIR CBIR 为基于内容的图像检索. 用于在图像数据数据库上检索具有</description>
    </item>
    
    <item>
      <title>超分辨率重建: SRGAN</title>
      <link>/post/srgan/</link>
      <pubDate>Sun, 12 Apr 2020 19:41:57 +0800</pubDate>
      
      <guid>/post/srgan/</guid>
      <description>对于图像超分辨率重建, 第一个使用CNN实现的是SRCNN, 类似于编码器解码器结构. SRGAN是第一个使用GAN网络解决超分辨率重构的网络 创新</description>
    </item>
    
    <item>
      <title>语义分割: UNET</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:46 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</guid>
      <description>说到语义分割, 不得不说一下U-net, U-net首先针对于医学图像分割提出, 由于其卓越的性能, 目前大部分医学图像分割都是基于U-net或者U</description>
    </item>
    
    <item>
      <title>语义分割: deeplab V1到deeplab V3</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:32 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</guid>
      <description>deeplab 为一个系列, 因此将其放在一起进行个回顾 Deeplab-v1 与deeplab-v2 将deeplab-v1与deeplab-v2放在一起, 主要是因为二者总体结构</description>
    </item>
    
    <item>
      <title>语义分割之: FCN</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:05 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</guid>
      <description>研究生阶段自己对分割这边还是很熟悉的, 工作后发现很多网络只能说出原理和整体框架, 面试时问的很细节, 再次将经典分割网络仔细review一遍. 主</description>
    </item>
    
    <item>
      <title>人脸检测网络: MTCNN</title>
      <link>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</link>
      <pubDate>Sat, 04 Apr 2020 18:03:03 +0800</pubDate>
      
      <guid>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</guid>
      <description>之前做人脸检测使用的是retinaface做的, 刚好最近被问到MTCNN, 以前没有细看, 正好做个笔记. MTCNN是2015年提出的用于人脸检</description>
    </item>
    
    <item>
      <title>图像去雾: Single Image Haze Removal Using Dark Channel Prior</title>
      <link>/post/%E6%9A%97%E9%80%9A%E9%81%93%E5%8E%BB%E9%9B%BEsingle_image_haze_he/</link>
      <pubDate>Fri, 03 Apr 2020 15:04:06 +0800</pubDate>
      
      <guid>/post/%E6%9A%97%E9%80%9A%E9%81%93%E5%8E%BB%E9%9B%BEsingle_image_haze_he/</guid>
      <description>此论文是何凯明博士在2009CVPR上的发表的论文, 在图像增强领域可谓无人不知. 论文的方法主要基于对无雾室外图像的统计. 本文主要对原文以及参</description>
    </item>
    
    <item>
      <title>Anchor free目标检测修炼之路:fcos -Fully Convolutional One-Stage Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</link>
      <pubDate>Tue, 10 Mar 2020 23:35:58 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</guid>
      <description>主流的目标检测算法大多数是基于anchor box的, one-stage 的yolo-v2, yolo-v3, ssd&amp;hellip;以及two-stage 的faster rcnn</description>
    </item>
    
    <item>
      <title>图像修复3: Free-Form Image Inpainting with Gated Convolution</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</link>
      <pubDate>Thu, 20 Feb 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</guid>
      <description>此文与图像修复2中review过的论文一样, 出至Adobe同一人之手. 主要创新点 提出门控卷积解决普通卷积将所有输入像素都视为有效像素的问题,</description>
    </item>
    
    <item>
      <title>OHEM: 在线困难样本挖掘: Training Region-based Object Detectors with Online Hard Example Mining</title>
      <link>/post/ohem/</link>
      <pubDate>Sun, 16 Feb 2020 18:46:23 +0800</pubDate>
      
      <guid>/post/ohem/</guid>
      <description>在目标检测中, 存在正负样本类别不平衡的现象, 特别是对于单阶段的目标检测算法. 如果每张训练图片中目标个数还很少的话, 背景区域就占了绝大部分, 分</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v2 损失函数实现讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 10 Feb 2020 16:20:36 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>最近闲暇时自己在pytorch实现并训练了yolo-v2, 对yolo-v2的实现做一个简单的总结, 主要是loss 层, 别的地方都没啥难度 关于y</description>
    </item>
    
    <item>
      <title>Accelerating Object Detection by Erasing Background Activations 阅读</title>
      <link>/post/omg_net/</link>
      <pubDate>Mon, 10 Feb 2020 13:10:54 +0800</pubDate>
      
      <guid>/post/omg_net/</guid>
      <description>该篇论文来源于Intel, 如其名用来加速目标检测. 主要针对于 one-stage 的目标检测算法. 主要创新点 对于 one-stage 的目标检测算法而言, 由于其设置了大量的 default box, 然后</description>
    </item>
    
    <item>
      <title>Focal loss </title>
      <link>/post/focal_loss/</link>
      <pubDate>Sun, 09 Feb 2020 18:54:07 +0800</pubDate>
      
      <guid>/post/focal_loss/</guid>
      <description>intro 主流的目标检测网络主要包含两种架构,一种是先进行region proposal再分别对得到的region 进行分类与边框回归的 two-stage 网络, RCNN 及其衍</description>
    </item>
    
    <item>
      <title>FPN: Feature Pyramid Networks for Object Detection </title>
      <link>/post/fpn/</link>
      <pubDate>Sun, 09 Feb 2020 14:42:44 +0800</pubDate>
      
      <guid>/post/fpn/</guid>
      <description>常见的检测网络结构 (a) 中使用图像金字塔构建特征金字塔, 在每一个图像尺度下单独提取特征, 耗时 (b) 使用单尺度特征图用来快速的目标检测 (c) 使用单方向多个</description>
    </item>
    
    <item>
      <title>目标检测与实例分割: Mask rcnn</title>
      <link>/post/mask_rcnn/</link>
      <pubDate>Fri, 07 Feb 2020 00:12:19 +0800</pubDate>
      
      <guid>/post/mask_rcnn/</guid>
      <description>mask rcnn 是何凯明团队在faster rcnn的基础上, 将目标检测与实例分割整合到一起的又一力作, 同时改进了faster RCNN 中 ROI pooling存在的 misalignment</description>
    </item>
    
    <item>
      <title>内容损失与风格损失</title>
      <link>/post/%E5%86%85%E5%AE%B9%E6%8D%9F%E5%A4%B1%E4%B8%8E%E9%A3%8E%E6%A0%BC%E6%8D%9F%E5%A4%B1/</link>
      <pubDate>Tue, 04 Feb 2020 19:15:55 +0800</pubDate>
      
      <guid>/post/%E5%86%85%E5%AE%B9%E6%8D%9F%E5%A4%B1%E4%B8%8E%E9%A3%8E%E6%A0%BC%E6%8D%9F%E5%A4%B1/</guid>
      <description>在画风迁移与超分辨率重建以及图像修复等视觉领域, 内容损失又称感知损失, 使用的较多, 在此做个记录, 同时也记录下风格损失 内容损失与提取计算 内容损</description>
    </item>
    
    <item>
      <title>Progan: PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION</title>
      <link>/post/progan%E9%98%85%E8%AF%BB/</link>
      <pubDate>Mon, 20 Jan 2020 22:09:52 +0800</pubDate>
      
      <guid>/post/progan%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 GAN 高清图像生成难的问题 方法 提出新的 GAN 训练方法: 逐渐增加生成器和鉴别器&amp;ndash;从低分辨率开始, 逐渐添加新层，以随着训练的进行网络</description>
    </item>
    
    <item>
      <title>Densecrf与图像分割</title>
      <link>/post/densecrf%E5%B0%8F%E8%AE%A1/</link>
      <pubDate>Thu, 02 Jan 2020 23:54:19 +0800</pubDate>
      
      <guid>/post/densecrf%E5%B0%8F%E8%AE%A1/</guid>
      <description>在图像分割中，在FCN之前，流行的是以概率图模型为代表的传统方法. FCN出来之后一段时间，仍然流行的是以FCN为前端，CRF为后端优化. 随着</description>
    </item>
    
    <item>
      <title>图像修复2: Generative Image Inpainting with Contextual Attention</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</link>
      <pubDate>Thu, 02 Jan 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</guid>
      <description>之前介绍过NVIDIA的图像修复方法, 使用的img-img的方法. 由于GAN网络在图像生成方向的大放异彩, 今天review一下, Adobe的</description>
    </item>
    
    <item>
      <title>深度学习归一化: BN,LN,IN,GN</title>
      <link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BD%92%E4%B8%80%E5%8C%96/</link>
      <pubDate>Sun, 29 Dec 2019 13:51:47 +0800</pubDate>
      
      <guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BD%92%E4%B8%80%E5%8C%96/</guid>
      <description>深度学习中的常见归一化方法主要有: batch Normalization, layer Normalization, instance Normalization, group Normalization. 神经网络学习过程的本质就是为了学习数据分布, 如果没有做归一化处理, 那么每一批次训练数据的分</description>
    </item>
    
    <item>
      <title>图像修复1: Image Inpainting for Irregular Holes Using Partial Convolutions</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</link>
      <pubDate>Sat, 28 Dec 2019 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</guid>
      <description>图像修复一直是CV领域的重点与难点, 基于深度学习的图像修复因为其可以学习到丰富的语义信息和潜在丰富的表达能力, 受到研究者们的热捧. 这篇文章主</description>
    </item>
    
    <item>
      <title>可分离卷积</title>
      <link>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Tue, 24 Dec 2019 22:09:22 +0800</pubDate>
      
      <guid>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</guid>
      <description>可分离卷积主要包括空间可分离卷积（Spatial Separable Convolutions）、深度可分离卷积（Depthwise Separable Convolutions.</description>
    </item>
    
    <item>
      <title>人脸检测网络: SSH: Single Stage Headless Face Detector</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Wed, 18 Dec 2019 22:54:51 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</guid>
      <description>最近看了一下人脸检测的论文, 除了通用的目标检测方法, 看见了这篇论文, 整体上而言和yolo-v3结构是类似的, SSH 设计了不同的检测头. SSH 网络 SSH 网</description>
    </item>
    
    <item>
      <title>Squeeze-and-Excitation Networks</title>
      <link>/post/se-net/</link>
      <pubDate>Wed, 18 Dec 2019 11:59:07 +0800</pubDate>
      
      <guid>/post/se-net/</guid>
      <description>Squeeze-and-Excitation Networks（SENet)是CVPR2018公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v1 实现关键点讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 10 Dec 2019 16:20:31 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>yolo-v1作为anchor free 的目标检测方法, 虽然已经较老，但深入理解其原理还是很有必要的. 对于个人而言, 完全从头实现目标检测算法是必不可</description>
    </item>
    
    <item>
      <title>Facenet: A Unified Embedding for Face Recognition and Clustering</title>
      <link>/post/facenet/</link>
      <pubDate>Thu, 28 Nov 2019 22:31:58 +0800</pubDate>
      
      <guid>/post/facenet/</guid>
      <description>主要创新点 FaceNet 将 face verification(判断是否是同一个人)，recognition(判断是何人)和clustering(寻找相似人脸) 任</description>
    </item>
    
    <item>
      <title>Dcgan: 深度卷积生成对抗网络</title>
      <link>/post/dcgan/</link>
      <pubDate>Tue, 26 Nov 2019 19:58:17 +0800</pubDate>
      
      <guid>/post/dcgan/</guid>
      <description>DCGAN 在 Ian Goodfellow 提出GAN 以来, 在图像领域GAN 可谓被玩的声名大噪. DCAGN 主要是针对卷积实现GAN时, 提出一系列架构设计规则, 使其训练更稳定. 主要有以下</description>
    </item>
    
    <item>
      <title>目标检测之RFB模块: Receptive Field Block Net for Accurate and Fast Object Detection</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Mon, 18 Nov 2019 22:02:39 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</guid>
      <description>RFB 模块主要是针对在保持轻量级网络的速度快、计算量小的情况下, 提升检测的精度, 模块如其名, 从感受野角度入手, 增强轻量级网络的特征表示, 主要用来</description>
    </item>
    
    <item>
      <title>Anchor free 目标检测修炼之路: DenseBox : Unifying Landmark Localization with End to End Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</link>
      <pubDate>Wed, 13 Nov 2019 22:16:21 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</guid>
      <description>DenseBox 是与yolo, faster rcnn同期的目标检测网络, 与yolo v1一样采用 anchor-free的思想, 网络结构采用FCN来实现目标检测 主要创新点</description>
    </item>
    
    <item>
      <title>Dlib 人脸相关实践</title>
      <link>/post/dlib%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Tue, 12 Nov 2019 12:58:31 +0800</pubDate>
      
      <guid>/post/dlib%E5%AE%9E%E8%B7%B5/</guid>
      <description>Dlib由C++编写，提供了和机器学习、数值计算、图模型算法、图像处理等领域相关的一系列功能, 对Python 也提供了便利的接口, 但C++ 版功</description>
    </item>
    
    <item>
      <title>Yolo: v1-v3</title>
      <link>/post/yolo/</link>
      <pubDate>Sat, 09 Nov 2019 17:16:26 +0800</pubDate>
      
      <guid>/post/yolo/</guid>
      <description>目标检测主要有两种实现，一是faster-rcnn为代表的proposal two-stage 系列，二是以YOLO为代表的one-stage 的回归网络. 主要区</description>
    </item>
    
    <item>
      <title>动手搭建神经网络之:简单联合分割、检测网络</title>
      <link>/post/how_to_built_a_simple_maskrcnn/</link>
      <pubDate>Mon, 28 Oct 2019 23:58:27 +0800</pubDate>
      
      <guid>/post/how_to_built_a_simple_maskrcnn/</guid>
      <description>coursera deeplearning.ai目标检测课后实践，构建一个简化版单目标yolo目标检测并添加前景对象分割分支 网络结构 MASK-Rcnn主要是</description>
    </item>
    
    <item>
      <title>Inception系列</title>
      <link>/post/inception_v1_to_v4/</link>
      <pubDate>Thu, 24 Oct 2019 16:51:27 +0800</pubDate>
      
      <guid>/post/inception_v1_to_v4/</guid>
      <description>对卷积神经网络而言，提升网络的深度与宽度能够显著提升网络的性能，但是网络越大意味着参数量的增加，会使网络更加容易过拟合。同时，增加网络的大小</description>
    </item>
    
    <item>
      <title>Object detection(4): Faster R-CNN</title>
      <link>/post/faster_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sun, 20 Oct 2019 22:10:11 +0800</pubDate>
      
      <guid>/post/faster_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 提出 RPN 网络, 将region proposal 和目标检测统一到一个卷积网络中 anchor 的使用 网络结构 faster rcnn 检测流程如上图所示: 图像经过卷积层提取 feature maps 然后在RPN</description>
    </item>
    
    <item>
      <title>SSD: Single Shot MultiBox Detector</title>
      <link>/post/ssd/</link>
      <pubDate>Fri, 18 Oct 2019 12:19:25 +0800</pubDate>
      
      <guid>/post/ssd/</guid>
      <description>SSD发表在2016ECCV, 是one-stage目标检测算法中经典的框架之一. 其精度优于yolo-v1, 在yolo-v2之后被超越. SSD 具有</description>
    </item>
    
    <item>
      <title>object detection(3): Fast rcnn</title>
      <link>/post/fast_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Wed, 09 Oct 2019 21:24:19 +0800</pubDate>
      
      <guid>/post/fast_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 RCNN, SPPnet的训练是 multi-stage的, 需要每一步训练一个模型. faste rcnn 通过 multi-task loss 将分类与边框回归融合到网络中, 前两者为训练SVM</description>
    </item>
    
    <item>
      <title>GRU小结</title>
      <link>/post/gru%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Mon, 07 Oct 2019 22:05:27 +0800</pubDate>
      
      <guid>/post/gru%E5%B0%8F%E7%BB%93/</guid>
      <description>GRU(Gated Recurrent Unit)即门控循环单元，是LSTM的变体. GRU保留了LSTM对梯度消失问题的抗力，但内部更简单. LSTM有：输入门，遗忘门，输出门，</description>
    </item>
    
    <item>
      <title>Object detection(2): SPPnet</title>
      <link>/post/spp_net/</link>
      <pubDate>Sat, 05 Oct 2019 19:40:34 +0800</pubDate>
      
      <guid>/post/spp_net/</guid>
      <description>在 RCNN 中说到, RCNN存在的一个问题是需要将region proposal进行warp到固定尺寸,这会带来失真等影响. 这是由于具有全连接的CNN</description>
    </item>
    
    <item>
      <title>object detection(1): Rcnn</title>
      <link>/post/rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Thu, 03 Oct 2019 21:24:12 +0800</pubDate>
      
      <guid>/post/rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>Rcnn 作为使用CNN进行目标检测的开山之作, 之后，在其基础上延展出了fast rcnn, faster rcnn, mask rcnn, 等等, 都是在针对前人的问题不断改进, 本文对rcnn 进行小结</description>
    </item>
    
    <item>
      <title>Anchor box</title>
      <link>/post/anchor_box/</link>
      <pubDate>Tue, 01 Oct 2019 15:17:30 +0800</pubDate>
      
      <guid>/post/anchor_box/</guid>
      <description>对于主流的 one-stage(Faster r-cnn&amp;hellip;) 或者是 tow-stage(SSD, YOLO&amp;hellip;)的目标检测算法, 大多都采用了 anchor/prior box机制. Anchor-box 的意义 在yolo-v1 中, 每个grid 输出两个b</description>
    </item>
    
    <item>
      <title>Edge_gan总结</title>
      <link>/post/edge_gan/</link>
      <pubDate>Thu, 19 Sep 2019 23:32:10 +0800</pubDate>
      
      <guid>/post/edge_gan/</guid>
      <description>最近刚好在做分割，顺手玩玩用GAN做边缘检测. 本意是想在BSDS轮廓分割数据集上做，同时验证针对样本极不平衡的损失函数挑选问题，简单做个小结</description>
    </item>
    
    <item>
      <title>L1、L2、Smooth_L1损失函数对比</title>
      <link>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 19 Sep 2019 11:07:59 +0800</pubDate>
      
      <guid>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</guid>
      <description>L1、L2以及Smooth_L1损失函数作为目标检测中回归层常用的损失函数，对他们进行一个对比分析. 三者公式对比如下: 分别对三者求导数: L2</description>
    </item>
    
    <item>
      <title>目标检测map计算</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bmap%E8%AE%A1%E7%AE%97/</link>
      <pubDate>Tue, 17 Sep 2019 20:23:17 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bmap%E8%AE%A1%E7%AE%97/</guid>
      <description>目标检测评价指标MAP计算流程小结 目标检测由于包含分类以及box回归, 对其进行评价相对于单独的分类问题更复杂，直接使用精度、召回作为评估准则</description>
    </item>
    
    <item>
      <title>从RNN到LSTM小记</title>
      <link>/post/lstm/</link>
      <pubDate>Fri, 13 Sep 2019 19:51:54 +0800</pubDate>
      
      <guid>/post/lstm/</guid>
      <description>记录自己对LSTM结构的理解，以及结合keras在实现LSTM模型时数据的输入数据等的处理。 1.SimpleRNN 对于多层感知机网络而言，是假设每个输入数据具有</description>
    </item>
    
    <item>
      <title>目标检测非极大值抑制: NMS</title>
      <link>/post/nms/</link>
      <pubDate>Thu, 12 Sep 2019 17:28:58 +0800</pubDate>
      
      <guid>/post/nms/</guid>
      <description>非极大值抑制（Non-Maximum Suppression，NMS），即抑制不是极大值的元素，可以理解为局部最大搜索. 在目标检测中, 无论是 one-stage</description>
    </item>
    
    <item>
      <title>CRNN笔记以及数字检测识别实践</title>
      <link>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 08 Sep 2019 16:07:07 +0800</pubDate>
      
      <guid>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</guid>
      <description>主流的OCR识别分为两个部分:先检测出文字区域再识别文字。检测可采用通用的目标检测方法以及针对于文本检测的网络,识别主要是CRNN及其变体。</description>
    </item>
    
    <item>
      <title>East:An Efficient and Accurate Scene Text Detector阅读及应用</title>
      <link>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 28 Aug 2019 00:03:07 +0800</pubDate>
      
      <guid>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</guid>
      <description>East是旷视科技2017年发表的论文,针对于场景文本检测。East网络也可以轻易的扩展到其他目标检测任务上。我主要在改进版的East基础上</description>
    </item>
    
    <item>
      <title>CycleGAN论文阅读总结及实现</title>
      <link>/post/cyclegan/</link>
      <pubDate>Sat, 24 Aug 2019 21:42:13 +0800</pubDate>
      
      <guid>/post/cyclegan/</guid>
      <description>在cyclegan之前，对于两个域的图像进行转化，比如图像风格转换，它们的训练集图像都是成对的.而cyclegan则解决了训练图像必须成对的</description>
    </item>
    
    <item>
      <title>基于vgg16的半监督视频单目标分割网络&#43; Dense-crf</title>
      <link>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</link>
      <pubDate>Sat, 24 Aug 2019 00:03:12 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</guid>
      <description>.img-wrap{ border: 1px } img{ float: left; width: 25%; height: 160; } one-shot 半监督视频单目标分割 网络实现 采用keras实现，网络结构如下。 类似于unet，但没有unet那么多的参数。使用de</description>
    </item>
    
    <item>
      <title>（译）你的神经网络不工作的37个可能原因</title>
      <link>/post/37reasonsforyournetnotwork/</link>
      <pubDate>Wed, 07 Aug 2019 11:29:13 +0800</pubDate>
      
      <guid>/post/37reasonsforyournetnotwork/</guid>
      <description>神经网络的训练是一个复杂的问题，很多时候会遇见即使拿到了别人的代码也训练不出来，无法复现。 以下是37个训练网络的建议英文原文： 1.最基本的措</description>
    </item>
    
    <item>
      <title>基于yolo_v3的水印检测</title>
      <link>/post/yolo%E6%B0%B4%E5%8D%B0%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Sun, 21 Jul 2019 21:25:20 +0800</pubDate>
      
      <guid>/post/yolo%E6%B0%B4%E5%8D%B0%E6%A3%80%E6%B5%8B/</guid>
      <description>背景 近年来版权意识的提高，在使用别人图片的时候（尤其是商业领域），需要检测图片是否有别的公司的水印（ 主要针对人眼可见的水印，除去数字加密等水</description>
    </item>
    
    <item>
      <title>视频对象分割小记</title>
      <link>/post/videoseg_summary/</link>
      <pubDate>Thu, 06 Jun 2019 15:25:02 +0800</pubDate>
      
      <guid>/post/videoseg_summary/</guid>
      <description>写在前面的话，硕士研究生阶段从接触VOS到深入研究，差不多一共有两年时间。因为自己刚接触这个研究领域的时候，用深度学习做视频分割的还相对较少</description>
    </item>
    
    <item>
      <title>图像处理: 双边滤波器</title>
      <link>/post/%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2%E5%99%A8/</link>
      <pubDate>Sat, 04 May 2019 10:50:37 +0800</pubDate>
      
      <guid>/post/%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2%E5%99%A8/</guid>
      <description>双边滤波器 百科 &amp;ldquo;双边滤波（Bilateral Filter）是非线性滤波中的一种。这是一种结合图像的空间邻近度与像素值相似度的处理</description>
    </item>
    
    <item>
      <title>Hr_net阅读笔记</title>
      <link>/post/hr_net/</link>
      <pubDate>Sat, 13 Apr 2019 14:52:14 +0800</pubDate>
      
      <guid>/post/hr_net/</guid>
      <description>HRNet 是中科大与微软亚洲研究院今年发表的关于人体姿态估计的论文中提出的网络结构。 我不是做姿态估计的，主要是HRNet的结构对于需要跨层特征融合以</description>
    </item>
    
    <item>
      <title>图像处理: 高斯滤波器</title>
      <link>/post/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8/</link>
      <pubDate>Sat, 06 Apr 2019 12:58:48 +0800</pubDate>
      
      <guid>/post/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8/</guid>
      <description>高斯滤波是一种线性平滑滤波，适用于消除高斯噪声，广泛应用于图像处理的减噪过程, 高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都</description>
    </item>
    
    <item>
      <title>Keras数据增强并保存到本地</title>
      <link>/post/keras_data_aug/</link>
      <pubDate>Fri, 29 Mar 2019 15:16:16 +0800</pubDate>
      
      <guid>/post/keras_data_aug/</guid>
      <description>当需要对指定文件夹下的图片进行数据增广时，使用keras的ImageDataGenerator类的flow_from_directory（）</description>
    </item>
    
    <item>
      <title>Keras多标签分类网络实现</title>
      <link>/post/keras_duofenlei/</link>
      <pubDate>Fri, 29 Mar 2019 15:07:11 +0800</pubDate>
      
      <guid>/post/keras_duofenlei/</guid>
      <description>简谈多分类与多标签分类 简单的说，输入一张图片进行分类： * 这张图片里面的物体（通常认为只有一个物体）属于某一个类，各个类别之间的概率是竞争关系</description>
    </item>
    
    <item>
      <title>Keras数据集加载小结</title>
      <link>/post/keras_dataload/</link>
      <pubDate>Tue, 26 Mar 2019 15:20:23 +0800</pubDate>
      
      <guid>/post/keras_dataload/</guid>
      <description>对于keras加载训练数据，官方上没有详说。然而网上查各种资料，写法太多，通过自己跑代码测试总结以下几条，方便自己以后使用。 总的来说kera</description>
    </item>
    
    <item>
      <title>python下mnist数据集转化为图片</title>
      <link>/post/mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%AC%E5%9B%BE%E7%89%87/</link>
      <pubDate>Sat, 22 Dec 2018 15:37:02 +0800</pubDate>
      
      <guid>/post/mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%AC%E5%9B%BE%E7%89%87/</guid>
      <description>环境：tensorflow 代码如下 from tensorflow.examples.tutorials.mnist import input_data from scipy import misc import numpy as np import os mnist = input_data.read_data_sets(&#39;MNIST_data/&#39;,one_hot=True) result_path =&#39;mnist_data\\train&#39; def onehot2id(labels): return list(labels).index(1) if not os.path.exists(result_path): os.mkdir(result_path) labels_txt = open(&#39;train_labs.txt&#39;,&#39;w&#39;) for i in range(len(mnist.train.images)): img_vec = mnist.train.images[i,:] img_arr = np.reshape(img_vec,[28,28]) img_lab = mnist.train.labels[i,:] img_id = onehot2id(img_lab) labels_txt.write(str(i)+&#39; &#39;+str(img_id)+&#39;\n&#39;) img_path = os.path.join(result_path,str(i)+&#39;.png&#39;) misc.imsave(img_path,img_arr) 以</description>
    </item>
    
    <item>
      <title>监督分类之：KNN算法</title>
      <link>/post/knn/</link>
      <pubDate>Sat, 22 Dec 2018 15:34:07 +0800</pubDate>
      
      <guid>/post/knn/</guid>
      <description>KNN简介 K近邻（K-Nearest Neighbor）学习是一种简单的监督学习方法。方法流程主要是：对于给定的测试样本，基于某种距离度量找出</description>
    </item>
    
    <item>
      <title>模版匹配之相关匹配</title>
      <link>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</link>
      <pubDate>Thu, 20 Dec 2018 15:44:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</guid>
      <description>模板匹配 最近准备把学过的一些知识整理写成博客，加深印象。 模板匹配是一种最原始、最基本的模式识别方法，研究某一特定对象物的图案位于图像的什么地</description>
    </item>
    
    <item>
      <title>线性拟合笔记之：Ransac算法</title>
      <link>/post/ransac/</link>
      <pubDate>Wed, 19 Dec 2018 15:52:22 +0800</pubDate>
      
      <guid>/post/ransac/</guid>
      <description>关于Ransac算法 RANSAC为Random Sample Consensus，即随机采样一致性算法，是根据一组包含异常数据的样本数据集，计算出数据的数</description>
    </item>
    
    <item>
      <title>线性拟合笔记之：最小二乘法</title>
      <link>/post/linersqur/</link>
      <pubDate>Tue, 18 Dec 2018 15:57:45 +0800</pubDate>
      
      <guid>/post/linersqur/</guid>
      <description>关于最小二乘法 以下是百度百科的解释：最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小</description>
    </item>
    
    <item>
      <title>Ubuntu &#43; Python下libsvm使用小结</title>
      <link>/post/libsvm%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 28 Nov 2018 16:24:49 +0800</pubDate>
      
      <guid>/post/libsvm%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93/</guid>
      <description>关于libsvm libsvm是台湾大学林智仁(Chih-Jen Lin)教授等开发，它主要用于分类(支持二分类和多分类)和回归，主页主页，下载</description>
    </item>
    
    <item>
      <title>基于Keras图像相似度计算孪生网络</title>
      <link>/post/keras_simi/</link>
      <pubDate>Mon, 12 Nov 2018 16:32:32 +0800</pubDate>
      
      <guid>/post/keras_simi/</guid>
      <description>import keras from keras.layers import Input,Dense,Conv2D from keras.layers import MaxPooling2D,Flatten,Convolution2D from keras.models import Model import os import numpy as np from PIL import Image from keras.optimizers import SGD from scipy import misc root_path = os.getcwd() train_names = [&#39;bear&#39;,&#39;blackswan&#39;,&#39;bus&#39;,&#39;camel&#39;,&#39;car&#39;,&#39;cows&#39;,&#39;dance&#39;,&#39;dog&#39;,&#39;hike&#39;,&#39;hoc&#39;,&#39;kite&#39;,&#39;lucia&#39;,&#39;mallerd&#39;,&#39;pigs&#39;,&#39;soapbox&#39;,&#39;stro&#39;,&#39;surf&#39;,&#39;swing&#39;,&#39;train&#39;,&#39;walking&#39;] test_names = [&#39;boat&#39;,&#39;dance-jump&#39;,&#39;drift-turn&#39;,&#39;elephant&#39;,&#39;libby&#39;] def load_data(seq_names,data_number,seq_len): #生成图片对 print(&#39;loading data.....&#39;) frame_num = 51 train_data1 = [] train_data2 = [] train_lab = [] count = 0 while count &amp;lt; data_number:</description>
    </item>
    
    <item>
      <title>Keras 猫狗二分类</title>
      <link>/post/kdog_cat/</link>
      <pubDate>Sun, 11 Nov 2018 16:36:27 +0800</pubDate>
      
      <guid>/post/kdog_cat/</guid>
      <description>import keras from keras.models import Sequential from keras.layers import Dense,MaxPooling2D,Input,Flatten,Convolution2D,Dropout,GlobalAveragePooling2D from keras.optimizers import SGD from keras.callbacks import TensorBoard,ModelCheckpoint from PIL import Image import os import numpy as np from scipy import misc root_path = os.getcwd() def load_data(): tran_imags = [] labels = [] seq_names = [&#39;cat&#39;,&#39;dog&#39;] for seq_name in seq_names: frames = sorted(os.listdir(os.path.join(root_path,&#39;data&#39;,&#39;train_data&#39;, seq_name))) for frame in frames: imgs = [os.path.join(root_path, &#39;data&#39;, &#39;train_data&#39;, seq_name, frame)] imgs = np.array(Image.open(imgs[0])) tran_imags.append(imgs) if</description>
    </item>
    
    <item>
      <title>图像相似度之PSNR与SSIM小结</title>
      <link>/post/psnr/</link>
      <pubDate>Wed, 03 Oct 2018 16:55:08 +0800</pubDate>
      
      <guid>/post/psnr/</guid>
      <description>PSNR（Peak Signal to Noise Ratio）：峰值信噪比 使用局部均值误差来判断差异，对于两个H*W*C的图像，I1,I2 其中n为采样值的比特数，比如</description>
    </item>
    
    <item>
      <title>图像联通区域标记</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E8%BF%9E%E9%80%9A%E5%8C%BA%E5%9F%9F%E6%A0%87%E8%AE%B0/</link>
      <pubDate>Fri, 21 Sep 2018 17:01:55 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E8%BF%9E%E9%80%9A%E5%8C%BA%E5%9F%9F%E6%A0%87%E8%AE%B0/</guid>
      <description>由于最近做实验用到二值图像连通区域（八连通）标记，刚开始的时候为了验证算法有效性，用了递归的方法（太慢了，而且图像一大就容易栈溢出），最后查</description>
    </item>
    
  </channel>
</rss>