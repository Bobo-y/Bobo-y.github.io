<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on fly away, chase dream</title>
    <link>/post/</link>
    <description>Recent content in Posts on fly away, chase dream</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 25 Jan 2022 17:22:37 +0800</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ViLD(Vision and Language knowledge Distillation): 基于视觉和语言模型的zero shot 目标检测</title>
      <link>/post/vild/</link>
      <pubDate>Tue, 25 Jan 2022 17:22:37 +0800</pubDate>
      
      <guid>/post/vild/</guid>
      <description>对于常用的目标检测而言，测试集和训练集的类别时保持一致的，即我们想要检测什么，那么训练集就有该类别的数据. 对于zero-shot 即测试集的出</description>
    </item>
    
    <item>
      <title>谷歌Detic: 结合分类数据集进行目标检测模型训练</title>
      <link>/post/zero-shotdetic/</link>
      <pubDate>Sat, 22 Jan 2022 21:58:46 +0800</pubDate>
      
      <guid>/post/zero-shotdetic/</guid>
      <description>论文三连问 论文做了什么：使用分类数据集来训练检测模型的分类器，使检测器可以识别出上万的类别 论文怎么做的：对于检测标注格式的数据和分类标注格式</description>
    </item>
    
    <item>
      <title>识别损失函数汇总</title>
      <link>/post/%E8%AF%86%E5%88%AB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/</link>
      <pubDate>Thu, 20 Jan 2022 23:30:02 +0800</pubDate>
      
      <guid>/post/%E8%AF%86%E5%88%AB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/</guid>
      <description>在之前的文章中记录了部分用于重识别的损失，主要是基于欧式距离的损失，本文接着对人脸识别使用的损失进一步做个小结. 首先，针对于识别任务，不论是</description>
    </item>
    
    <item>
      <title>CLIP: Learning Transferable Visual Models From Natural Language Supervision理解与使用小记</title>
      <link>/post/clip%E5%B0%8F%E8%AE%B0/</link>
      <pubDate>Tue, 18 Jan 2022 14:15:11 +0800</pubDate>
      
      <guid>/post/clip%E5%B0%8F%E8%AE%B0/</guid>
      <description>CLIP 是 openAI 提出的用以将图像映射到文本描述空间中，连接图片和文本，可以用来提取图像 embedding, 用作zero-shot 迁移。 CLIP 结构 CLIP 总体结构如上图： 通过对比学</description>
    </item>
    
    <item>
      <title>对比学习之SimCLR与MoCo小记</title>
      <link>/post/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E4%B9%8Bsimclr%E4%B8%8Emoco%E5%B0%8F%E8%AE%B0/</link>
      <pubDate>Tue, 11 Jan 2022 18:04:11 +0800</pubDate>
      
      <guid>/post/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E4%B9%8Bsimclr%E4%B8%8Emoco%E5%B0%8F%E8%AE%B0/</guid>
      <description>SimCLR 与 MOCO 都是采用自监督、对比学习的形式来训练视觉模型。因为对于主流的CNN网络，模型训练都依赖于人工标注，但是人工标注成本太大，我们能使用的标</description>
    </item>
    
    <item>
      <title>Masked Autoencoders Are Scalable Vision Learners小记</title>
      <link>/post/mae/</link>
      <pubDate>Mon, 03 Jan 2022 22:13:10 +0800</pubDate>
      
      <guid>/post/mae/</guid>
      <description>读前三问 论文做了什么：论文以自监督的形式来训练自动编码器用来提取特征，实现无标注的预训练 怎么做的：对输入图片进行mask，采用编码器-解码器</description>
    </item>
    
    <item>
      <title>Yolox阅读笔记</title>
      <link>/post/yolox%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 08 Oct 2021 21:27:28 +0800</pubDate>
      
      <guid>/post/yolox%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>yolox 是旷世今年推出的一个新的YOLO检测器技术报告，核心是将YOLO与anchor free方式实现，性能超过了之前的YOLO系列。此笔记记录Y</description>
    </item>
    
    <item>
      <title>Pix2seq阅读笔记</title>
      <link>/post/pix2seq%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 08 Oct 2021 21:27:22 +0800</pubDate>
      
      <guid>/post/pix2seq%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>灵魂三问 论文做了什么? 该论文是谷歌最近的新作,以语言建模的形式实现目标检测。 论文怎么做的？ 将bounding box 和 类别标签离散化为token。</description>
    </item>
    
    <item>
      <title>Swin Transformer：层次化视觉Transformer 笔记</title>
      <link>/post/swin_transoformer/</link>
      <pubDate>Tue, 01 Jun 2021 19:33:06 +0800</pubDate>
      
      <guid>/post/swin_transoformer/</guid>
      <description>cver 不读swin transformer,遍读transformer也枉然. 个人读完论文感觉最大贡献在于: 将多尺度引入到了transformer</description>
    </item>
    
    <item>
      <title>京东人脸识别FaceX Zoo</title>
      <link>/post/%E4%BA%AC%E4%B8%9C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%ABfacex-zoo/</link>
      <pubDate>Thu, 20 May 2021 14:17:05 +0800</pubDate>
      
      <guid>/post/%E4%BA%AC%E4%B8%9C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%ABfacex-zoo/</guid>
      <description>FaceX-Zoo 是京东最近开源的人脸识别pytorch版工具箱, 抽空读了一下技术报告，做个记录. 工程总体结构 总体可分为训练模块、评估模块、附加模块和人脸S</description>
    </item>
    
    <item>
      <title>Ppyolov2 笔记</title>
      <link>/post/ppyolov2/</link>
      <pubDate>Wed, 19 May 2021 11:08:19 +0800</pubDate>
      
      <guid>/post/ppyolov2/</guid>
      <description>PP-YOLOv2 是百度对于ppyolo-v1 的升级版，主要是引入了各种插件来提升性能。如下图，ppyolov2 在相同的map下能达到更高的FPS. PPyolo-v2 的改进</description>
    </item>
    
    <item>
      <title>经典无监督聚类算法理论温习与Python代码实践</title>
      <link>/post/%E7%BB%8F%E5%85%B8%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E6%B8%A9%E4%B9%A0%E4%B8%8Epython%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Wed, 28 Apr 2021 10:39:39 +0800</pubDate>
      
      <guid>/post/%E7%BB%8F%E5%85%B8%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA%E6%B8%A9%E4%B9%A0%E4%B8%8Epython%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/</guid>
      <description>在不知道样本类别时，无监督聚类算法可以将具有相似属性或特征的样本归为相同的类别，常用语数据处理. 本文主要记录几种常见的聚类算法以及Pytho</description>
    </item>
    
    <item>
      <title>Reid之路: 度量学习的几种主要loss</title>
      <link>/post/reid%E4%B9%8B%E8%B7%AF-loss/</link>
      <pubDate>Wed, 17 Mar 2021 22:41:32 +0800</pubDate>
      
      <guid>/post/reid%E4%B9%8B%E8%B7%AF-loss/</guid>
      <description>度量学习旨在学习两张图片的相似性，那么久需要设计合适的损失函数能让网络提取到更具有判别能力的特征. contrasitive loss 网络优化目标是最小化损失函数 $L _c$, 输入正</description>
    </item>
    
    <item>
      <title>分布式训练基础</title>
      <link>/post/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 17 Mar 2021 22:28:09 +0800</pubDate>
      
      <guid>/post/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80/</guid>
      <description>对于算法工程师而言，单卡跑不起来或者跑起来慢是很常见的问题，分布式训练各个框架都有支持。有时候训练模型一张卡放得下但误以为多卡一定就快，训练</description>
    </item>
    
    <item>
      <title>Detr:End-to-End Object Detection with Transformers笔记</title>
      <link>/post/detr%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sun, 07 Mar 2021 15:56:46 +0800</pubDate>
      
      <guid>/post/detr%E7%AC%94%E8%AE%B0/</guid>
      <description>目前transformer在CV领域打的火热，前文记录了transformer用于图像分类的实现，本文主要记录transformer用于目标</description>
    </item>
    
    <item>
      <title>Vision Transformer 笔记</title>
      <link>/post/vit%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 06 Mar 2021 22:11:57 +0800</pubDate>
      
      <guid>/post/vit%E7%AC%94%E8%AE%B0/</guid>
      <description>本文主要从代码角度记录使用transformer实现图像分类的流程. 代码vit-pytorch/ 总体结构 结合上图与代码展开: 前向传播过程代码</description>
    </item>
    
    <item>
      <title>Reid之路:Deep Learning for Person Re-identification:A Survey and Outlook</title>
      <link>/post/reid%E4%B9%8B%E8%B7%AF-survey/</link>
      <pubDate>Mon, 01 Mar 2021 23:30:57 +0800</pubDate>
      
      <guid>/post/reid%E4%B9%8B%E8%B7%AF-survey/</guid>
      <description>目前团队项目逐渐偏向识别，自己的工作重心逐渐由检测、分类转向reid相关，自己之前对reid也没有深入，因此准备记录自己在reid工作上的成</description>
    </item>
    
    <item>
      <title>Transformer 个人小结</title>
      <link>/post/transformer%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/</link>
      <pubDate>Thu, 25 Feb 2021 21:47:37 +0800</pubDate>
      
      <guid>/post/transformer%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/</guid>
      <description>transformer 已经出来三年了，自己也没有深入了解，触及皮毛，最近想研究一下detr, 顺便写个 📒 记录一下对transformer的理解，个人主要从代码角度</description>
    </item>
    
    <item>
      <title>Yolov4.Pytorch 代码学习笔记</title>
      <link>/post/yolov4-pytorch/</link>
      <pubDate>Tue, 16 Feb 2021 22:33:31 +0800</pubDate>
      
      <guid>/post/yolov4-pytorch/</guid>
      <description>最近对yolov5进行了较为深入的理解，顺便将yolov4给啃一啃，之前只粗略读过论文，这边文章主要从代码进行学习，代码参照 pytorch版</description>
    </item>
    
    <item>
      <title>PP-Yolo阅读笔记</title>
      <link>/post/pp-yolo/</link>
      <pubDate>Mon, 15 Feb 2021 23:15:21 +0800</pubDate>
      
      <guid>/post/pp-yolo/</guid>
      <description>PP-YOLO 是百度在paddle-paddle框架下基于YOLOv3,结合各种trick得到的一个在性能与效率平衡的检测网络。与yolov4、effi</description>
    </item>
    
    <item>
      <title>Yolov5收敛快的理解</title>
      <link>/post/yolov5%E6%94%B6%E6%95%9B%E5%BF%AB%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <pubDate>Sun, 27 Dec 2020 22:02:02 +0800</pubDate>
      
      <guid>/post/yolov5%E6%94%B6%E6%95%9B%E5%BF%AB%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <description>训练过很多目标检测网络，个人觉得yolov5是使用过的收敛最快的网络，训练10多个epoch就能达到很高的P、R. 收敛快的原因也不是网络本身</description>
    </item>
    
    <item>
      <title>Yolo-v5从代码到服务部署实践</title>
      <link>/post/yolo_v5%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Fri, 25 Dec 2020 15:27:39 +0800</pubDate>
      
      <guid>/post/yolo_v5%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/</guid>
      <description>yolo-v5 非论文，仅工程实现。本文主要记录自己对yolo-v5代码的学习、理解，以及实际服务部署。 网络结构 yolo-v5 包含4种模型结构，分别是yolov5s、</description>
    </item>
    
    <item>
      <title>PP-Ocr阅读小结</title>
      <link>/post/pp-ocr%E9%98%85%E8%AF%BB%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 11 Nov 2020 22:14:40 +0800</pubDate>
      
      <guid>/post/pp-ocr%E9%98%85%E8%AF%BB%E5%B0%8F%E7%BB%93/</guid>
      <description>PP-OCR 是百度基于paddlePaddle 框架开源的国产高质量的OCR系统，PP-OCR 论文主要对其中使用的技术作了介绍。本文对PP-OCR 作阅读</description>
    </item>
    
    <item>
      <title>使用nginx实现服务反向代理实现负载均衡</title>
      <link>/post/%E4%BD%BF%E7%94%A8nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%AE%9E%E7%8E%B0%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Tue, 06 Oct 2020 14:10:30 +0800</pubDate>
      
      <guid>/post/%E4%BD%BF%E7%94%A8nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%AE%9E%E7%8E%B0%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>对外提供网络接口服务，当单机容量达到极限时，可以从业务拆分和分布式部署两个方面进行分析，来解决接口访问量大，并发量高，海量数据的问题。从单机</description>
    </item>
    
    <item>
      <title>Dbnet</title>
      <link>/post/dbnet/</link>
      <pubDate>Mon, 03 Aug 2020 21:35:33 +0800</pubDate>
      
      <guid>/post/dbnet/</guid>
      <description>DBNet：Real-time Scene Text Detection with Differentiable Binarization，是一个基于分割的文本检测器，PPOCR中使用其作为检测器，取得了可观的效果</description>
    </item>
    
    <item>
      <title>CSPNet</title>
      <link>/post/cspnet/</link>
      <pubDate>Sun, 26 Jul 2020 22:46:55 +0800</pubDate>
      
      <guid>/post/cspnet/</guid>
      <description>CSPNet出至论文：CSPNet: A New Backbone that can Enhance Learning Capability of CNN, 近来yolo-v4,yolo-v5都使用其作为主干网络的结构，其主要用于降低计算量的</description>
    </item>
    
    <item>
      <title>CBAM注意力模块: Convolutional Block Attention Module</title>
      <link>/post/cbam%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Mon, 11 May 2020 11:27:35 +0800</pubDate>
      
      <guid>/post/cbam%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97/</guid>
      <description>之前谈过SE-net, 对于目标检测或检测用于特征通道的attention, 今天记录一下CBAM模块, 对分类或检测中用来获取通道、空间位置的a</description>
    </item>
    
    <item>
      <title>Image Caption模型</title>
      <link>/post/image_caption_1/</link>
      <pubDate>Sun, 03 May 2020 16:23:18 +0800</pubDate>
      
      <guid>/post/image_caption_1/</guid>
      <description>图像描述生成作为结合CV与NLP的跨模态学习任务, 在人工智能领域也是热门的研究点. 模型 Image caption 是在给定照片的情况下生成人类可读的文字描述的具有挑</description>
    </item>
    
    <item>
      <title>Seq2Seq模型: 从理论到实践(二)</title>
      <link>/post/seq2seq%E6%A8%A1%E5%9E%8B_%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5_%E4%BA%8C/</link>
      <pubDate>Sat, 02 May 2020 17:03:16 +0800</pubDate>
      
      <guid>/post/seq2seq%E6%A8%A1%E5%9E%8B_%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E5%AE%9E%E8%B7%B5_%E4%BA%8C/</guid>
      <description>本文图片、代码来源于pytorch-se12seq, 加深个人理解 在上文中使用的编码器-解码器结构如下: （一）信息压缩问题 对于上诉的解码器而言</description>
    </item>
    
    <item>
      <title>Seq2Seq模型: 从理论到实践(一)</title>
      <link>/post/seq2seq%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Fri, 01 May 2020 17:31:29 +0800</pubDate>
      
      <guid>/post/seq2seq%E6%A8%A1%E5%9E%8B/</guid>
      <description>理论结合实践是学习的最佳方式, 本文图片、代码来源于pytorch-seq2seq Seq2Seq 模型 对于序列预测, RNN及其变种LSTM、GRU等无疑是最</description>
    </item>
    
    <item>
      <title>Pytorch: CRNN 实践</title>
      <link>/post/pytorch_crnn%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Thu, 30 Apr 2020 16:49:49 +0800</pubDate>
      
      <guid>/post/pytorch_crnn%E5%AE%9E%E8%B7%B5/</guid>
      <description>最近开始深入OCR这块, 以前倒是训练过开源的Keras-CRNN, 但是它和原文还是不一样, 今天参照Keras-CRNN代码和CRNN论文用p</description>
    </item>
    
    <item>
      <title>文本检测中的nms</title>
      <link>/post/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84nms/</link>
      <pubDate>Wed, 29 Apr 2020 17:20:52 +0800</pubDate>
      
      <guid>/post/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84nms/</guid>
      <description>今天被问到了OCR相关的NMS，个人一直偏向于通用目标检测的NMS，正好补补课，扩展一下OCR方向的知识. 对通用目标检测或者人脸检测等得到的</description>
    </item>
    
    <item>
      <title>YOLOv4: Optimal Speed and Accuracy of Object Detection论文解读</title>
      <link>/post/yolov4/</link>
      <pubDate>Tue, 28 Apr 2020 23:37:42 +0800</pubDate>
      
      <guid>/post/yolov4/</guid>
      <description>最近目标检测又出了yolo-v4，作为一个做目标检测的不可不膜拜膜拜。首先由于约瑟夫大神已经退出CV，yolo-v4 的一作是DarkNet的</description>
    </item>
    
    <item>
      <title>目标检测IOU评价指标汇总: GIOU, DIOU, CIOU</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</link>
      <pubDate>Fri, 24 Apr 2020 23:18:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</guid>
      <description>在目标检测中, IOU 可以被用来评估预测框的性能，IOU越大预测框越准。IOU可表示两个框的距离，IOU越大距离越小. 对于目标检测坐标损失虽然一般</description>
    </item>
    
    <item>
      <title>谷歌最新目标检测论文: EfficientDet</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</link>
      <pubDate>Sat, 18 Apr 2020 11:55:48 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</guid>
      <description>最近谷歌放出了 EfficientDet 论文与代码, 在COCO上取得了最好的MAP, 本文对 efficientDet 做个简要的总结, 同时对efficientNet也做个回顾. Efficie</description>
    </item>
    
    <item>
      <title>基于内容的图像检索: pytorch</title>
      <link>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</link>
      <pubDate>Fri, 17 Apr 2020 16:33:21 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</guid>
      <description>看了Jason Brownlee博士的Keras CBIR demo, 自己也动手用pytorch写一个. CBIR CBIR 为基于内容的图像检索. 用于在图像数据数据库上检索具有</description>
    </item>
    
    <item>
      <title>超分辨率重建: SRGAN</title>
      <link>/post/srgan/</link>
      <pubDate>Sun, 12 Apr 2020 19:41:57 +0800</pubDate>
      
      <guid>/post/srgan/</guid>
      <description>对于图像超分辨率重建, 第一个使用CNN实现的是SRCNN, 类似于编码器解码器结构. SRGAN是第一个使用GAN网络解决超分辨率重构的网络 创新</description>
    </item>
    
    <item>
      <title>语义分割: UNET</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:46 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</guid>
      <description>说到语义分割, 不得不说一下U-net, U-net首先针对于医学图像分割提出, 由于其卓越的性能, 目前大部分医学图像分割都是基于U-net或者U</description>
    </item>
    
    <item>
      <title>语义分割: deeplab V1到deeplab V3</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:32 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</guid>
      <description>deeplab 为一个系列, 因此将其放在一起进行个回顾 Deeplab-v1 与deeplab-v2 将deeplab-v1与deeplab-v2放在一起, 主要是因为二者总体结构</description>
    </item>
    
    <item>
      <title>语义分割之: FCN</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:05 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</guid>
      <description>研究生阶段自己对分割这边还是很熟悉的, 工作后发现很多网络只能说出原理和整体框架, 面试时问的很细节, 再次将经典分割网络仔细review一遍. 主</description>
    </item>
    
    <item>
      <title>人脸检测网络: MTCNN</title>
      <link>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</link>
      <pubDate>Sat, 04 Apr 2020 18:03:03 +0800</pubDate>
      
      <guid>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</guid>
      <description>之前做人脸检测使用的是retinaface做的, 刚好最近被问到MTCNN, 以前没有细看, 正好做个笔记. MTCNN是2015年提出的用于人脸检</description>
    </item>
    
    <item>
      <title>图像去雾: Single Image Haze Removal Using Dark Channel Prior</title>
      <link>/post/%E6%9A%97%E9%80%9A%E9%81%93%E5%8E%BB%E9%9B%BEsingle_image_haze_he/</link>
      <pubDate>Fri, 03 Apr 2020 15:04:06 +0800</pubDate>
      
      <guid>/post/%E6%9A%97%E9%80%9A%E9%81%93%E5%8E%BB%E9%9B%BEsingle_image_haze_he/</guid>
      <description>此论文是何凯明博士在2009CVPR上的发表的论文, 在图像增强领域可谓无人不知. 论文的方法主要基于对无雾室外图像的统计. 本文主要对原文以及参</description>
    </item>
    
    <item>
      <title>Anchor free目标检测修炼之路:fcos -Fully Convolutional One-Stage Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</link>
      <pubDate>Tue, 10 Mar 2020 23:35:58 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</guid>
      <description>主流的目标检测算法大多数是基于anchor box的, one-stage 的yolo-v2, yolo-v3, ssd&amp;hellip;以及two-stage 的faster rcnn</description>
    </item>
    
    <item>
      <title>图像修复3: Free-Form Image Inpainting with Gated Convolution</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</link>
      <pubDate>Thu, 20 Feb 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</guid>
      <description>此文与图像修复2中review过的论文一样, 出至Adobe同一人之手. 主要创新点 提出门控卷积解决普通卷积将所有输入像素都视为有效像素的问题,</description>
    </item>
    
    <item>
      <title>OHEM: 在线困难样本挖掘: Training Region-based Object Detectors with Online Hard Example Mining</title>
      <link>/post/ohem/</link>
      <pubDate>Sun, 16 Feb 2020 18:46:23 +0800</pubDate>
      
      <guid>/post/ohem/</guid>
      <description>在目标检测中, 存在正负样本类别不平衡的现象, 特别是对于单阶段的目标检测算法. 如果每张训练图片中目标个数还很少的话, 背景区域就占了绝大部分, 分</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v2 损失函数实现讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 10 Feb 2020 16:20:36 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>最近闲暇时自己在pytorch实现并训练了yolo-v2, 对yolo-v2的实现做一个简单的总结, 主要是loss 层, 别的地方都没啥难度 关于y</description>
    </item>
    
    <item>
      <title>Accelerating Object Detection by Erasing Background Activations 阅读</title>
      <link>/post/omg_net/</link>
      <pubDate>Mon, 10 Feb 2020 13:10:54 +0800</pubDate>
      
      <guid>/post/omg_net/</guid>
      <description>该篇论文来源于Intel, 如其名用来加速目标检测. 主要针对于 one-stage 的目标检测算法. 主要创新点 对于 one-stage 的目标检测算法而言, 由于其设置了大量的 default box, 然后</description>
    </item>
    
    <item>
      <title>Focal loss </title>
      <link>/post/focal_loss/</link>
      <pubDate>Sun, 09 Feb 2020 18:54:07 +0800</pubDate>
      
      <guid>/post/focal_loss/</guid>
      <description>intro 主流的目标检测网络主要包含两种架构,一种是先进行region proposal再分别对得到的region 进行分类与边框回归的 two-stage 网络, RCNN 及其衍</description>
    </item>
    
    <item>
      <title>FPN: Feature Pyramid Networks for Object Detection </title>
      <link>/post/fpn/</link>
      <pubDate>Sun, 09 Feb 2020 14:42:44 +0800</pubDate>
      
      <guid>/post/fpn/</guid>
      <description>常见的检测网络结构 (a) 中使用图像金字塔构建特征金字塔, 在每一个图像尺度下单独提取特征, 耗时 (b) 使用单尺度特征图用来快速的目标检测 (c) 使用单方向多个</description>
    </item>
    
    <item>
      <title>目标检测与实例分割: Mask rcnn</title>
      <link>/post/mask_rcnn/</link>
      <pubDate>Fri, 07 Feb 2020 00:12:19 +0800</pubDate>
      
      <guid>/post/mask_rcnn/</guid>
      <description>mask rcnn 是何凯明团队在faster rcnn的基础上, 将目标检测与实例分割整合到一起的又一力作, 同时改进了faster RCNN 中 ROI pooling存在的 misalignment</description>
    </item>
    
    <item>
      <title>内容损失与风格损失</title>
      <link>/post/%E5%86%85%E5%AE%B9%E6%8D%9F%E5%A4%B1%E4%B8%8E%E9%A3%8E%E6%A0%BC%E6%8D%9F%E5%A4%B1/</link>
      <pubDate>Tue, 04 Feb 2020 19:15:55 +0800</pubDate>
      
      <guid>/post/%E5%86%85%E5%AE%B9%E6%8D%9F%E5%A4%B1%E4%B8%8E%E9%A3%8E%E6%A0%BC%E6%8D%9F%E5%A4%B1/</guid>
      <description>在画风迁移与超分辨率重建以及图像修复等视觉领域, 内容损失又称感知损失, 使用的较多, 在此做个记录, 同时也记录下风格损失 内容损失与提取计算 内容损</description>
    </item>
    
    <item>
      <title>Progan: PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION</title>
      <link>/post/progan%E9%98%85%E8%AF%BB/</link>
      <pubDate>Mon, 20 Jan 2020 22:09:52 +0800</pubDate>
      
      <guid>/post/progan%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 GAN 高清图像生成难的问题 方法 提出新的 GAN 训练方法: 逐渐增加生成器和鉴别器&amp;ndash;从低分辨率开始, 逐渐添加新层，以随着训练的进行网络</description>
    </item>
    
    <item>
      <title>Densecrf与图像分割</title>
      <link>/post/densecrf%E5%B0%8F%E8%AE%A1/</link>
      <pubDate>Thu, 02 Jan 2020 23:54:19 +0800</pubDate>
      
      <guid>/post/densecrf%E5%B0%8F%E8%AE%A1/</guid>
      <description>在图像分割中，在FCN之前，流行的是以概率图模型为代表的传统方法. FCN出来之后一段时间，仍然流行的是以FCN为前端，CRF为后端优化. 随着</description>
    </item>
    
    <item>
      <title>图像修复2: Generative Image Inpainting with Contextual Attention</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</link>
      <pubDate>Thu, 02 Jan 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</guid>
      <description>之前介绍过NVIDIA的图像修复方法, 使用的img-img的方法. 由于GAN网络在图像生成方向的大放异彩, 今天review一下, Adobe的</description>
    </item>
    
    <item>
      <title>深度学习归一化: BN,LN,IN,GN</title>
      <link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BD%92%E4%B8%80%E5%8C%96/</link>
      <pubDate>Sun, 29 Dec 2019 13:51:47 +0800</pubDate>
      
      <guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BD%92%E4%B8%80%E5%8C%96/</guid>
      <description>深度学习中的常见归一化方法主要有: batch Normalization, layer Normalization, instance Normalization, group Normalization. 神经网络学习过程的本质就是为了学习数据分布, 如果没有做归一化处理, 那么每一批次训练数据的分</description>
    </item>
    
    <item>
      <title>图像修复1: Image Inpainting for Irregular Holes Using Partial Convolutions</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</link>
      <pubDate>Sat, 28 Dec 2019 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</guid>
      <description>图像修复一直是CV领域的重点与难点, 基于深度学习的图像修复因为其可以学习到丰富的语义信息和潜在丰富的表达能力, 受到研究者们的热捧. 这篇文章主</description>
    </item>
    
    <item>
      <title>libtorch c&#43;&#43; 使用例子</title>
      <link>/post/pytorch%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Wed, 25 Dec 2019 12:37:07 +0800</pubDate>
      
      <guid>/post/pytorch%E9%83%A8%E7%BD%B2/</guid>
      <description>pytorch可使用flask作为服务器部署，但是由于Python的可移植性和速度比不上c++, pytorch还提供将模型转化到c++端运行</description>
    </item>
    
    <item>
      <title>可分离卷积</title>
      <link>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Tue, 24 Dec 2019 22:09:22 +0800</pubDate>
      
      <guid>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</guid>
      <description>可分离卷积主要包括空间可分离卷积（Spatial Separable Convolutions）、深度可分离卷积（Depthwise Separable Convolutions.</description>
    </item>
    
    <item>
      <title>人脸检测网络: SSH: Single Stage Headless Face Detector</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Wed, 18 Dec 2019 22:54:51 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</guid>
      <description>最近看了一下人脸检测的论文, 除了通用的目标检测方法, 看见了这篇论文, 整体上而言和yolo-v3结构是类似的, SSH 设计了不同的检测头. SSH 网络 SSH 网</description>
    </item>
    
    <item>
      <title>Squeeze-and-Excitation Networks</title>
      <link>/post/se-net/</link>
      <pubDate>Wed, 18 Dec 2019 11:59:07 +0800</pubDate>
      
      <guid>/post/se-net/</guid>
      <description>Squeeze-and-Excitation Networks（SENet)是CVPR2018公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v1 实现关键点讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 10 Dec 2019 16:20:31 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>yolo-v1作为anchor free 的目标检测方法, 虽然已经较老，但深入理解其原理还是很有必要的. 对于个人而言, 完全从头实现目标检测算法是必不可</description>
    </item>
    
    <item>
      <title>Facenet: A Unified Embedding for Face Recognition and Clustering</title>
      <link>/post/facenet/</link>
      <pubDate>Thu, 28 Nov 2019 22:31:58 +0800</pubDate>
      
      <guid>/post/facenet/</guid>
      <description>主要创新点 FaceNet 将 face verification(判断是否是同一个人)，recognition(判断是何人)和clustering(寻找相似人脸) 任</description>
    </item>
    
    <item>
      <title>Dcgan: 深度卷积生成对抗网络</title>
      <link>/post/dcgan/</link>
      <pubDate>Tue, 26 Nov 2019 19:58:17 +0800</pubDate>
      
      <guid>/post/dcgan/</guid>
      <description>DCGAN 在 Ian Goodfellow 提出GAN 以来, 在图像领域GAN 可谓被玩的声名大噪. DCAGN 主要是针对卷积实现GAN时, 提出一系列架构设计规则, 使其训练更稳定. 主要有以下</description>
    </item>
    
    <item>
      <title>目标检测之RFB模块: Receptive Field Block Net for Accurate and Fast Object Detection</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Mon, 18 Nov 2019 22:02:39 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</guid>
      <description>RFB 模块主要是针对在保持轻量级网络的速度快、计算量小的情况下, 提升检测的精度, 模块如其名, 从感受野角度入手, 增强轻量级网络的特征表示, 主要用来</description>
    </item>
    
    <item>
      <title>Anchor free 目标检测修炼之路: DenseBox : Unifying Landmark Localization with End to End Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</link>
      <pubDate>Wed, 13 Nov 2019 22:16:21 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</guid>
      <description>DenseBox 是与yolo, faster rcnn同期的目标检测网络, 与yolo v1一样采用 anchor-free的思想, 网络结构采用FCN来实现目标检测 主要创新点</description>
    </item>
    
    <item>
      <title>Dlib 人脸相关实践</title>
      <link>/post/dlib%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Tue, 12 Nov 2019 12:58:31 +0800</pubDate>
      
      <guid>/post/dlib%E5%AE%9E%E8%B7%B5/</guid>
      <description>Dlib由C++编写，提供了和机器学习、数值计算、图模型算法、图像处理等领域相关的一系列功能, 对Python 也提供了便利的接口, 但C++ 版功</description>
    </item>
    
    <item>
      <title>Yolo: v1-v3</title>
      <link>/post/yolo/</link>
      <pubDate>Sat, 09 Nov 2019 17:16:26 +0800</pubDate>
      
      <guid>/post/yolo/</guid>
      <description>目标检测主要有两种实现，一是faster-rcnn为代表的proposal two-stage 系列，二是以YOLO为代表的one-stage 的回归网络. 主要区</description>
    </item>
    
    <item>
      <title>动手搭建神经网络之:简单联合分割、检测网络</title>
      <link>/post/how_to_built_a_simple_maskrcnn/</link>
      <pubDate>Mon, 28 Oct 2019 23:58:27 +0800</pubDate>
      
      <guid>/post/how_to_built_a_simple_maskrcnn/</guid>
      <description>coursera deeplearning.ai目标检测课后实践，构建一个简化版单目标yolo目标检测并添加前景对象分割分支 网络结构 MASK-Rcnn主要是</description>
    </item>
    
    <item>
      <title>Inception系列</title>
      <link>/post/inception_v1_to_v4/</link>
      <pubDate>Thu, 24 Oct 2019 16:51:27 +0800</pubDate>
      
      <guid>/post/inception_v1_to_v4/</guid>
      <description>对卷积神经网络而言，提升网络的深度与宽度能够显著提升网络的性能，但是网络越大意味着参数量的增加，会使网络更加容易过拟合。同时，增加网络的大小</description>
    </item>
    
    <item>
      <title>Object detection(4): Faster R-CNN</title>
      <link>/post/faster_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sun, 20 Oct 2019 22:10:11 +0800</pubDate>
      
      <guid>/post/faster_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 提出 RPN 网络, 将region proposal 和目标检测统一到一个卷积网络中 anchor 的使用 网络结构 faster rcnn 检测流程如上图所示: 图像经过卷积层提取 feature maps 然后在RPN</description>
    </item>
    
    <item>
      <title>SSD: Single Shot MultiBox Detector</title>
      <link>/post/ssd/</link>
      <pubDate>Fri, 18 Oct 2019 12:19:25 +0800</pubDate>
      
      <guid>/post/ssd/</guid>
      <description>SSD发表在2016ECCV, 是one-stage目标检测算法中经典的框架之一. 其精度优于yolo-v1, 在yolo-v2之后被超越. SSD 具有</description>
    </item>
    
    <item>
      <title>object detection(3): Fast rcnn</title>
      <link>/post/fast_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Wed, 09 Oct 2019 21:24:19 +0800</pubDate>
      
      <guid>/post/fast_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 RCNN, SPPnet的训练是 multi-stage的, 需要每一步训练一个模型. faste rcnn 通过 multi-task loss 将分类与边框回归融合到网络中, 前两者为训练SVM</description>
    </item>
    
    <item>
      <title>GRU小结</title>
      <link>/post/gru%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Mon, 07 Oct 2019 22:05:27 +0800</pubDate>
      
      <guid>/post/gru%E5%B0%8F%E7%BB%93/</guid>
      <description>GRU(Gated Recurrent Unit)即门控循环单元，是LSTM的变体. GRU保留了LSTM对梯度消失问题的抗力，但内部更简单. LSTM有：输入门，遗忘门，输出门，</description>
    </item>
    
    <item>
      <title>Object detection(2): SPPnet</title>
      <link>/post/spp_net/</link>
      <pubDate>Sat, 05 Oct 2019 19:40:34 +0800</pubDate>
      
      <guid>/post/spp_net/</guid>
      <description>在 RCNN 中说到, RCNN存在的一个问题是需要将region proposal进行warp到固定尺寸,这会带来失真等影响. 这是由于具有全连接的CNN</description>
    </item>
    
    <item>
      <title>object detection(1): Rcnn</title>
      <link>/post/rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Thu, 03 Oct 2019 21:24:12 +0800</pubDate>
      
      <guid>/post/rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>Rcnn 作为使用CNN进行目标检测的开山之作, 之后，在其基础上延展出了fast rcnn, faster rcnn, mask rcnn, 等等, 都是在针对前人的问题不断改进, 本文对rcnn 进行小结</description>
    </item>
    
    <item>
      <title>Anchor box</title>
      <link>/post/anchor_box/</link>
      <pubDate>Tue, 01 Oct 2019 15:17:30 +0800</pubDate>
      
      <guid>/post/anchor_box/</guid>
      <description>对于主流的 one-stage(Faster r-cnn&amp;hellip;) 或者是 tow-stage(SSD, YOLO&amp;hellip;)的目标检测算法, 大多都采用了 anchor/prior box机制. Anchor-box 的意义 在yolo-v1 中, 每个grid 输出两个b</description>
    </item>
    
    <item>
      <title>Edge_gan总结</title>
      <link>/post/edge_gan/</link>
      <pubDate>Thu, 19 Sep 2019 23:32:10 +0800</pubDate>
      
      <guid>/post/edge_gan/</guid>
      <description>最近刚好在做分割，顺手玩玩用GAN做边缘检测. 本意是想在BSDS轮廓分割数据集上做，同时验证针对样本极不平衡的损失函数挑选问题，简单做个小结</description>
    </item>
    
    <item>
      <title>L1、L2、Smooth_L1损失函数对比</title>
      <link>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 19 Sep 2019 11:07:59 +0800</pubDate>
      
      <guid>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</guid>
      <description>L1、L2以及Smooth_L1损失函数作为目标检测中回归层常用的损失函数，对他们进行一个对比分析. 三者公式对比如下: 分别对三者求导数: L2</description>
    </item>
    
    <item>
      <title>目标检测map计算</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bmap%E8%AE%A1%E7%AE%97/</link>
      <pubDate>Tue, 17 Sep 2019 20:23:17 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bmap%E8%AE%A1%E7%AE%97/</guid>
      <description>目标检测评价指标MAP计算流程小结 目标检测由于包含分类以及box回归, 对其进行评价相对于单独的分类问题更复杂，直接使用精度、召回作为评估准则</description>
    </item>
    
    <item>
      <title>从RNN到LSTM小记</title>
      <link>/post/lstm/</link>
      <pubDate>Fri, 13 Sep 2019 19:51:54 +0800</pubDate>
      
      <guid>/post/lstm/</guid>
      <description>记录自己对LSTM结构的理解，以及结合keras在实现LSTM模型时数据的输入数据等的处理。 1.SimpleRNN 对于多层感知机网络而言，是假设每个输入数据具有</description>
    </item>
    
    <item>
      <title>目标检测非极大值抑制: NMS</title>
      <link>/post/nms/</link>
      <pubDate>Thu, 12 Sep 2019 17:28:58 +0800</pubDate>
      
      <guid>/post/nms/</guid>
      <description>非极大值抑制（Non-Maximum Suppression，NMS），即抑制不是极大值的元素，可以理解为局部最大搜索. 在目标检测中, 无论是 one-stage</description>
    </item>
    
    <item>
      <title>CRNN笔记以及数字检测识别实践</title>
      <link>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 08 Sep 2019 16:07:07 +0800</pubDate>
      
      <guid>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</guid>
      <description>主流的OCR识别分为两个部分:先检测出文字区域再识别文字。检测可采用通用的目标检测方法以及针对于文本检测的网络,识别主要是CRNN及其变体。</description>
    </item>
    
    <item>
      <title>East:An Efficient and Accurate Scene Text Detector阅读及应用</title>
      <link>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 28 Aug 2019 00:03:07 +0800</pubDate>
      
      <guid>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</guid>
      <description>East是旷视科技2017年发表的论文,针对于场景文本检测。East网络也可以轻易的扩展到其他目标检测任务上。我主要在改进版的East基础上</description>
    </item>
    
    <item>
      <title>CycleGAN论文阅读总结及实现</title>
      <link>/post/cyclegan/</link>
      <pubDate>Sat, 24 Aug 2019 21:42:13 +0800</pubDate>
      
      <guid>/post/cyclegan/</guid>
      <description>在cyclegan之前，对于两个域的图像进行转化，比如图像风格转换，它们的训练集图像都是成对的.而cyclegan则解决了训练图像必须成对的</description>
    </item>
    
    <item>
      <title>基于vgg16的半监督视频单目标分割网络&#43; Dense-crf</title>
      <link>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</link>
      <pubDate>Sat, 24 Aug 2019 00:03:12 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</guid>
      <description>.img-wrap{ border: 1px } img{ float: left; width: 25%; height: 160; } one-shot 半监督视频单目标分割 网络实现 采用keras实现，网络结构如下。 类似于unet，但没有unet那么多的参数。使用de</description>
    </item>
    
    <item>
      <title>（译）你的神经网络不工作的37个可能原因</title>
      <link>/post/37reasonsforyournetnotwork/</link>
      <pubDate>Wed, 07 Aug 2019 11:29:13 +0800</pubDate>
      
      <guid>/post/37reasonsforyournetnotwork/</guid>
      <description>神经网络的训练是一个复杂的问题，很多时候会遇见即使拿到了别人的代码也训练不出来，无法复现。 以下是37个训练网络的建议英文原文： 1.最基本的措</description>
    </item>
    
    <item>
      <title>基于yolo_v3的水印检测</title>
      <link>/post/yolo%E6%B0%B4%E5%8D%B0%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Sun, 21 Jul 2019 21:25:20 +0800</pubDate>
      
      <guid>/post/yolo%E6%B0%B4%E5%8D%B0%E6%A3%80%E6%B5%8B/</guid>
      <description>背景 近年来版权意识的提高，在使用别人图片的时候（尤其是商业领域），需要检测图片是否有别的公司的水印（ 主要针对人眼可见的水印，除去数字加密等水</description>
    </item>
    
    <item>
      <title>视频对象分割小记</title>
      <link>/post/videoseg_summary/</link>
      <pubDate>Thu, 06 Jun 2019 15:25:02 +0800</pubDate>
      
      <guid>/post/videoseg_summary/</guid>
      <description>写在前面的话，硕士研究生阶段从接触VOS到深入研究，差不多一共有两年时间。因为自己刚接触这个研究领域的时候，用深度学习做视频分割的还相对较少</description>
    </item>
    
    <item>
      <title>图像处理: 双边滤波器</title>
      <link>/post/%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2%E5%99%A8/</link>
      <pubDate>Sat, 04 May 2019 10:50:37 +0800</pubDate>
      
      <guid>/post/%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2%E5%99%A8/</guid>
      <description>双边滤波器 百科 &amp;ldquo;双边滤波（Bilateral Filter）是非线性滤波中的一种。这是一种结合图像的空间邻近度与像素值相似度的处理</description>
    </item>
    
    <item>
      <title>Hr_net阅读笔记</title>
      <link>/post/hr_net/</link>
      <pubDate>Sat, 13 Apr 2019 14:52:14 +0800</pubDate>
      
      <guid>/post/hr_net/</guid>
      <description>HRNet 是中科大与微软亚洲研究院今年发表的关于人体姿态估计的论文中提出的网络结构。 我不是做姿态估计的，主要是HRNet的结构对于需要跨层特征融合以</description>
    </item>
    
    <item>
      <title>图像处理: 高斯滤波器</title>
      <link>/post/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8/</link>
      <pubDate>Sat, 06 Apr 2019 12:58:48 +0800</pubDate>
      
      <guid>/post/%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8/</guid>
      <description>高斯滤波是一种线性平滑滤波，适用于消除高斯噪声，广泛应用于图像处理的减噪过程, 高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都</description>
    </item>
    
    <item>
      <title>Keras数据增强并保存到本地</title>
      <link>/post/keras_data_aug/</link>
      <pubDate>Fri, 29 Mar 2019 15:16:16 +0800</pubDate>
      
      <guid>/post/keras_data_aug/</guid>
      <description>当需要对指定文件夹下的图片进行数据增广时，使用keras的ImageDataGenerator类的flow_from_directory（）</description>
    </item>
    
    <item>
      <title>Keras多标签分类网络实现</title>
      <link>/post/keras_duofenlei/</link>
      <pubDate>Fri, 29 Mar 2019 15:07:11 +0800</pubDate>
      
      <guid>/post/keras_duofenlei/</guid>
      <description>简谈多分类与多标签分类 简单的说，输入一张图片进行分类： * 这张图片里面的物体（通常认为只有一个物体）属于某一个类，各个类别之间的概率是竞争关系</description>
    </item>
    
    <item>
      <title>Keras数据集加载小结</title>
      <link>/post/keras_dataload/</link>
      <pubDate>Tue, 26 Mar 2019 15:20:23 +0800</pubDate>
      
      <guid>/post/keras_dataload/</guid>
      <description>对于keras加载训练数据，官方上没有详说。然而网上查各种资料，写法太多，通过自己跑代码测试总结以下几条，方便自己以后使用。 总的来说kera</description>
    </item>
    
    <item>
      <title>python下mnist数据集转化为图片</title>
      <link>/post/mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%AC%E5%9B%BE%E7%89%87/</link>
      <pubDate>Sat, 22 Dec 2018 15:37:02 +0800</pubDate>
      
      <guid>/post/mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BD%AC%E5%9B%BE%E7%89%87/</guid>
      <description>环境：tensorflow 代码如下 from tensorflow.examples.tutorials.mnist import input_data from scipy import misc import numpy as np import os mnist = input_data.read_data_sets(&#39;MNIST_data/&#39;,one_hot=True) result_path =&#39;mnist_data\\train&#39; def onehot2id(labels): return list(labels).index(1) if not os.path.exists(result_path): os.mkdir(result_path) labels_txt = open(&#39;train_labs.txt&#39;,&#39;w&#39;) for i in range(len(mnist.train.images)): img_vec = mnist.train.images[i,:] img_arr = np.reshape(img_vec,[28,28]) img_lab = mnist.train.labels[i,:] img_id = onehot2id(img_lab) labels_txt.write(str(i)+&#39; &#39;+str(img_id)+&#39;\n&#39;) img_path = os.path.join(result_path,str(i)+&#39;.png&#39;) misc.imsave(img_path,img_arr) 以</description>
    </item>
    
    <item>
      <title>监督分类之：KNN算法</title>
      <link>/post/knn/</link>
      <pubDate>Sat, 22 Dec 2018 15:34:07 +0800</pubDate>
      
      <guid>/post/knn/</guid>
      <description>KNN简介 K近邻（K-Nearest Neighbor）学习是一种简单的监督学习方法。方法流程主要是：对于给定的测试样本，基于某种距离度量找出</description>
    </item>
    
    <item>
      <title>模版匹配之相关匹配</title>
      <link>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</link>
      <pubDate>Thu, 20 Dec 2018 15:44:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</guid>
      <description>模板匹配 最近准备把学过的一些知识整理写成博客，加深印象。 模板匹配是一种最原始、最基本的模式识别方法，研究某一特定对象物的图案位于图像的什么地</description>
    </item>
    
    <item>
      <title>线性拟合笔记之：Ransac算法</title>
      <link>/post/ransac/</link>
      <pubDate>Wed, 19 Dec 2018 15:52:22 +0800</pubDate>
      
      <guid>/post/ransac/</guid>
      <description>关于Ransac算法 RANSAC为Random Sample Consensus，即随机采样一致性算法，是根据一组包含异常数据的样本数据集，计算出数据的数</description>
    </item>
    
    <item>
      <title>线性拟合笔记之：最小二乘法</title>
      <link>/post/linersqur/</link>
      <pubDate>Tue, 18 Dec 2018 15:57:45 +0800</pubDate>
      
      <guid>/post/linersqur/</guid>
      <description>关于最小二乘法 以下是百度百科的解释：最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小</description>
    </item>
    
    <item>
      <title>Ubuntu &#43; Python下libsvm使用小结</title>
      <link>/post/libsvm%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 28 Nov 2018 16:24:49 +0800</pubDate>
      
      <guid>/post/libsvm%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93/</guid>
      <description>关于libsvm libsvm是台湾大学林智仁(Chih-Jen Lin)教授等开发，它主要用于分类(支持二分类和多分类)和回归，主页主页，下载</description>
    </item>
    
    <item>
      <title>基于Keras图像相似度计算孪生网络</title>
      <link>/post/keras_simi/</link>
      <pubDate>Mon, 12 Nov 2018 16:32:32 +0800</pubDate>
      
      <guid>/post/keras_simi/</guid>
      <description>import keras from keras.layers import Input,Dense,Conv2D from keras.layers import MaxPooling2D,Flatten,Convolution2D from keras.models import Model import os import numpy as np from PIL import Image from keras.optimizers import SGD from scipy import misc root_path = os.getcwd() train_names = [&#39;bear&#39;,&#39;blackswan&#39;,&#39;bus&#39;,&#39;camel&#39;,&#39;car&#39;,&#39;cows&#39;,&#39;dance&#39;,&#39;dog&#39;,&#39;hike&#39;,&#39;hoc&#39;,&#39;kite&#39;,&#39;lucia&#39;,&#39;mallerd&#39;,&#39;pigs&#39;,&#39;soapbox&#39;,&#39;stro&#39;,&#39;surf&#39;,&#39;swing&#39;,&#39;train&#39;,&#39;walking&#39;] test_names = [&#39;boat&#39;,&#39;dance-jump&#39;,&#39;drift-turn&#39;,&#39;elephant&#39;,&#39;libby&#39;] def load_data(seq_names,data_number,seq_len): #生成图片对 print(&#39;loading data.....&#39;) frame_num = 51 train_data1 = [] train_data2 = [] train_lab = [] count = 0 while count &amp;lt; data_number:</description>
    </item>
    
    <item>
      <title>Keras 猫狗二分类</title>
      <link>/post/kdog_cat/</link>
      <pubDate>Sun, 11 Nov 2018 16:36:27 +0800</pubDate>
      
      <guid>/post/kdog_cat/</guid>
      <description>import keras from keras.models import Sequential from keras.layers import Dense,MaxPooling2D,Input,Flatten,Convolution2D,Dropout,GlobalAveragePooling2D from keras.optimizers import SGD from keras.callbacks import TensorBoard,ModelCheckpoint from PIL import Image import os import numpy as np from scipy import misc root_path = os.getcwd() def load_data(): tran_imags = [] labels = [] seq_names = [&#39;cat&#39;,&#39;dog&#39;] for seq_name in seq_names: frames = sorted(os.listdir(os.path.join(root_path,&#39;data&#39;,&#39;train_data&#39;, seq_name))) for frame in frames: imgs = [os.path.join(root_path, &#39;data&#39;, &#39;train_data&#39;, seq_name, frame)] imgs = np.array(Image.open(imgs[0])) tran_imags.append(imgs) if</description>
    </item>
    
    <item>
      <title>图像相似度之PSNR与SSIM小结</title>
      <link>/post/psnr/</link>
      <pubDate>Wed, 03 Oct 2018 16:55:08 +0800</pubDate>
      
      <guid>/post/psnr/</guid>
      <description>PSNR（Peak Signal to Noise Ratio）：峰值信噪比 使用局部均值误差来判断差异，对于两个H*W*C的图像，I1,I2 其中n为采样值的比特数，比如</description>
    </item>
    
    <item>
      <title>图像联通区域标记</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E8%BF%9E%E9%80%9A%E5%8C%BA%E5%9F%9F%E6%A0%87%E8%AE%B0/</link>
      <pubDate>Fri, 21 Sep 2018 17:01:55 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E8%BF%9E%E9%80%9A%E5%8C%BA%E5%9F%9F%E6%A0%87%E8%AE%B0/</guid>
      <description>由于最近做实验用到二值图像连通区域（八连通）标记，刚开始的时候为了验证算法有效性，用了递归的方法（太慢了，而且图像一大就容易栈溢出），最后查</description>
    </item>
    
  </channel>
</rss>