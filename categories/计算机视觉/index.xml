<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>计算机视觉 on fly away, chase dream</title>
    <link>/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link>
    <description>Recent content in 计算机视觉 on fly away, chase dream</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 08 Oct 2021 21:27:22 +0800</lastBuildDate>
    
	<atom:link href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pix2seq阅读笔记</title>
      <link>/post/pix2seq%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 08 Oct 2021 21:27:22 +0800</pubDate>
      
      <guid>/post/pix2seq%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <description>灵魂三问 论文做了什么? 该论文是谷歌最近的新作,以语言建模的形式实现目标检测。 论文怎么做的？ 将bounding box 和 类别标签离散化为token。</description>
    </item>
    
    <item>
      <title>Swin Transformer：层次化视觉Transformer 笔记</title>
      <link>/post/swin_transoformer/</link>
      <pubDate>Tue, 01 Jun 2021 19:33:06 +0800</pubDate>
      
      <guid>/post/swin_transoformer/</guid>
      <description>cver 不读swin transformer,遍读transformer也枉然. 个人读完论文感觉最大贡献在于: 将多尺度引入到了transformer</description>
    </item>
    
    <item>
      <title>京东人脸识别FaceX Zoo</title>
      <link>/post/%E4%BA%AC%E4%B8%9C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%ABfacex-zoo/</link>
      <pubDate>Thu, 20 May 2021 14:17:05 +0800</pubDate>
      
      <guid>/post/%E4%BA%AC%E4%B8%9C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%ABfacex-zoo/</guid>
      <description>FaceX-Zoo 是京东最近开源的人脸识别pytorch版工具箱, 抽空读了一下技术报告，做个记录. 工程总体结构 总体可分为训练模块、评估模块、附加模块和人脸S</description>
    </item>
    
    <item>
      <title>Reid之路: 度量学习的几种主要loss</title>
      <link>/post/reid%E4%B9%8B%E8%B7%AF-loss/</link>
      <pubDate>Wed, 17 Mar 2021 22:41:32 +0800</pubDate>
      
      <guid>/post/reid%E4%B9%8B%E8%B7%AF-loss/</guid>
      <description>度量学习旨在学习两张图片的相似性，那么久需要设计合适的损失函数能让网络提取到更具有判别能力的特征. contrasitive loss 网络优化目标是最小化损失函数 $L _c$, 输入正</description>
    </item>
    
    <item>
      <title>Detr:End-to-End Object Detection with Transformers笔记</title>
      <link>/post/detr%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sun, 07 Mar 2021 15:56:46 +0800</pubDate>
      
      <guid>/post/detr%E7%AC%94%E8%AE%B0/</guid>
      <description>目前transformer在CV领域打的火热，前文记录了transformer用于图像分类的实现，本文主要记录transformer用于目标</description>
    </item>
    
    <item>
      <title>Vision Transformer 笔记</title>
      <link>/post/vit%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 06 Mar 2021 22:11:57 +0800</pubDate>
      
      <guid>/post/vit%E7%AC%94%E8%AE%B0/</guid>
      <description>本文主要从代码角度记录使用transformer实现图像分类的流程. 代码vit-pytorch/ 总体结构 结合上图与代码展开: 前向传播过程代码</description>
    </item>
    
    <item>
      <title>Reid之路:Deep Learning for Person Re-identification:A Survey and Outlook</title>
      <link>/post/reid%E4%B9%8B%E8%B7%AF-survey/</link>
      <pubDate>Mon, 01 Mar 2021 23:30:57 +0800</pubDate>
      
      <guid>/post/reid%E4%B9%8B%E8%B7%AF-survey/</guid>
      <description>目前团队项目逐渐偏向识别，自己的工作重心逐渐由检测、分类转向reid相关，自己之前对reid也没有深入，因此准备记录自己在reid工作上的成</description>
    </item>
    
    <item>
      <title>Yolov4.Pytorch 代码学习笔记</title>
      <link>/post/yolov4-pytorch/</link>
      <pubDate>Tue, 16 Feb 2021 22:33:31 +0800</pubDate>
      
      <guid>/post/yolov4-pytorch/</guid>
      <description>最近对yolov5进行了较为深入的理解，顺便将yolov4给啃一啃，之前只粗略读过论文，这边文章主要从代码进行学习，代码参照 pytorch版</description>
    </item>
    
    <item>
      <title>PP-Yolo阅读笔记</title>
      <link>/post/pp-yolo/</link>
      <pubDate>Mon, 15 Feb 2021 23:15:21 +0800</pubDate>
      
      <guid>/post/pp-yolo/</guid>
      <description>PP-YOLO 是百度在paddle-paddle框架下基于YOLOv3,结合各种trick得到的一个在性能与效率平衡的检测网络。与yolov4、effi</description>
    </item>
    
    <item>
      <title>Yolov5收敛快的理解</title>
      <link>/post/yolov5%E6%94%B6%E6%95%9B%E5%BF%AB%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <pubDate>Sun, 27 Dec 2020 22:02:02 +0800</pubDate>
      
      <guid>/post/yolov5%E6%94%B6%E6%95%9B%E5%BF%AB%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <description>训练过很多目标检测网络，个人觉得yolov5是使用过的收敛最快的网络，训练10多个epoch就能达到很高的P、R. 收敛快的原因也不是网络本身</description>
    </item>
    
    <item>
      <title>Yolo-v5从代码到服务部署实践</title>
      <link>/post/yolo_v5%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Fri, 25 Dec 2020 15:27:39 +0800</pubDate>
      
      <guid>/post/yolo_v5%E4%BB%8E%E7%90%86%E8%AE%BA%E5%88%B0%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/</guid>
      <description>yolo-v5 非论文，仅工程实现。本文主要记录自己对yolo-v5代码的学习、理解，以及实际服务部署。 网络结构 yolo-v5 包含4种模型结构，分别是yolov5s、</description>
    </item>
    
    <item>
      <title>PP-Ocr阅读小结</title>
      <link>/post/pp-ocr%E9%98%85%E8%AF%BB%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Wed, 11 Nov 2020 22:14:40 +0800</pubDate>
      
      <guid>/post/pp-ocr%E9%98%85%E8%AF%BB%E5%B0%8F%E7%BB%93/</guid>
      <description>PP-OCR 是百度基于paddlePaddle 框架开源的国产高质量的OCR系统，PP-OCR 论文主要对其中使用的技术作了介绍。本文对PP-OCR 作阅读</description>
    </item>
    
    <item>
      <title>Dbnet</title>
      <link>/post/dbnet/</link>
      <pubDate>Mon, 03 Aug 2020 21:35:33 +0800</pubDate>
      
      <guid>/post/dbnet/</guid>
      <description>DBNet：Real-time Scene Text Detection with Differentiable Binarization，是一个基于分割的文本检测器，PPOCR中使用其作为检测器，取得了可观的效果</description>
    </item>
    
    <item>
      <title>CSPNet</title>
      <link>/post/cspnet/</link>
      <pubDate>Sun, 26 Jul 2020 22:46:55 +0800</pubDate>
      
      <guid>/post/cspnet/</guid>
      <description>CSPNet出至论文：CSPNet: A New Backbone that can Enhance Learning Capability of CNN, 近来yolo-v4,yolo-v5都使用其作为主干网络的结构，其主要用于降低计算量的</description>
    </item>
    
    <item>
      <title>CBAM注意力模块: Convolutional Block Attention Module</title>
      <link>/post/cbam%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Mon, 11 May 2020 11:27:35 +0800</pubDate>
      
      <guid>/post/cbam%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97/</guid>
      <description>之前谈过SE-net, 对于目标检测或检测用于特征通道的attention, 今天记录一下CBAM模块, 对分类或检测中用来获取通道、空间位置的a</description>
    </item>
    
    <item>
      <title>Image Caption模型</title>
      <link>/post/image_caption_1/</link>
      <pubDate>Sun, 03 May 2020 16:23:18 +0800</pubDate>
      
      <guid>/post/image_caption_1/</guid>
      <description>图像描述生成作为结合CV与NLP的跨模态学习任务, 在人工智能领域也是热门的研究点. 模型 Image caption 是在给定照片的情况下生成人类可读的文字描述的具有挑</description>
    </item>
    
    <item>
      <title>Pytorch: CRNN 实践</title>
      <link>/post/pytorch_crnn%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Thu, 30 Apr 2020 16:49:49 +0800</pubDate>
      
      <guid>/post/pytorch_crnn%E5%AE%9E%E8%B7%B5/</guid>
      <description>最近开始深入OCR这块, 以前倒是训练过开源的Keras-CRNN, 但是它和原文还是不一样, 今天参照Keras-CRNN代码和CRNN论文用p</description>
    </item>
    
    <item>
      <title>文本检测中的nms</title>
      <link>/post/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84nms/</link>
      <pubDate>Wed, 29 Apr 2020 17:20:52 +0800</pubDate>
      
      <guid>/post/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84nms/</guid>
      <description>今天被问到了OCR相关的NMS，个人一直偏向于通用目标检测的NMS，正好补补课，扩展一下OCR方向的知识. 对通用目标检测或者人脸检测等得到的</description>
    </item>
    
    <item>
      <title>YOLOv4: Optimal Speed and Accuracy of Object Detection论文解读</title>
      <link>/post/yolov4/</link>
      <pubDate>Tue, 28 Apr 2020 23:37:42 +0800</pubDate>
      
      <guid>/post/yolov4/</guid>
      <description>最近目标检测又出了yolo-v4，作为一个做目标检测的不可不膜拜膜拜。首先由于约瑟夫大神已经退出CV，yolo-v4 的一作是DarkNet的</description>
    </item>
    
    <item>
      <title>目标检测IOU评价指标汇总: GIOU, DIOU, CIOU</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</link>
      <pubDate>Fri, 24 Apr 2020 23:18:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</guid>
      <description>在目标检测中, IOU 可以被用来评估预测框的性能，IOU越大预测框越准。IOU可表示两个框的距离，IOU越大距离越小. 对于目标检测坐标损失虽然一般</description>
    </item>
    
    <item>
      <title>谷歌最新目标检测论文: EfficientDet</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</link>
      <pubDate>Sat, 18 Apr 2020 11:55:48 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</guid>
      <description>最近谷歌放出了 EfficientDet 论文与代码, 在COCO上取得了最好的MAP, 本文对 efficientDet 做个简要的总结, 同时对efficientNet也做个回顾. Efficie</description>
    </item>
    
    <item>
      <title>基于内容的图像检索: pytorch</title>
      <link>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</link>
      <pubDate>Fri, 17 Apr 2020 16:33:21 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</guid>
      <description>看了Jason Brownlee博士的Keras CBIR demo, 自己也动手用pytorch写一个. CBIR CBIR 为基于内容的图像检索. 用于在图像数据数据库上检索具有</description>
    </item>
    
    <item>
      <title>超分辨率重建: SRGAN</title>
      <link>/post/srgan/</link>
      <pubDate>Sun, 12 Apr 2020 19:41:57 +0800</pubDate>
      
      <guid>/post/srgan/</guid>
      <description>对于图像超分辨率重建, 第一个使用CNN实现的是SRCNN, 类似于编码器解码器结构. SRGAN是第一个使用GAN网络解决超分辨率重构的网络 创新</description>
    </item>
    
    <item>
      <title>语义分割: UNET</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:46 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</guid>
      <description>说到语义分割, 不得不说一下U-net, U-net首先针对于医学图像分割提出, 由于其卓越的性能, 目前大部分医学图像分割都是基于U-net或者U</description>
    </item>
    
    <item>
      <title>语义分割: deeplab V1到deeplab V3</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:32 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</guid>
      <description>deeplab 为一个系列, 因此将其放在一起进行个回顾 Deeplab-v1 与deeplab-v2 将deeplab-v1与deeplab-v2放在一起, 主要是因为二者总体结构</description>
    </item>
    
    <item>
      <title>语义分割之: FCN</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:05 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</guid>
      <description>研究生阶段自己对分割这边还是很熟悉的, 工作后发现很多网络只能说出原理和整体框架, 面试时问的很细节, 再次将经典分割网络仔细review一遍. 主</description>
    </item>
    
    <item>
      <title>人脸检测网络: MTCNN</title>
      <link>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</link>
      <pubDate>Sat, 04 Apr 2020 18:03:03 +0800</pubDate>
      
      <guid>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</guid>
      <description>之前做人脸检测使用的是retinaface做的, 刚好最近被问到MTCNN, 以前没有细看, 正好做个笔记. MTCNN是2015年提出的用于人脸检</description>
    </item>
    
    <item>
      <title>Anchor free目标检测修炼之路:fcos -Fully Convolutional One-Stage Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</link>
      <pubDate>Tue, 10 Mar 2020 23:35:58 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</guid>
      <description>主流的目标检测算法大多数是基于anchor box的, one-stage 的yolo-v2, yolo-v3, ssd&amp;hellip;以及two-stage 的faster rcnn</description>
    </item>
    
    <item>
      <title>图像修复3: Free-Form Image Inpainting with Gated Convolution</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</link>
      <pubDate>Thu, 20 Feb 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</guid>
      <description>此文与图像修复2中review过的论文一样, 出至Adobe同一人之手. 主要创新点 提出门控卷积解决普通卷积将所有输入像素都视为有效像素的问题,</description>
    </item>
    
    <item>
      <title>OHEM: 在线困难样本挖掘: Training Region-based Object Detectors with Online Hard Example Mining</title>
      <link>/post/ohem/</link>
      <pubDate>Sun, 16 Feb 2020 18:46:23 +0800</pubDate>
      
      <guid>/post/ohem/</guid>
      <description>在目标检测中, 存在正负样本类别不平衡的现象, 特别是对于单阶段的目标检测算法. 如果每张训练图片中目标个数还很少的话, 背景区域就占了绝大部分, 分</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v2 损失函数实现讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 10 Feb 2020 16:20:36 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>最近闲暇时自己在pytorch实现并训练了yolo-v2, 对yolo-v2的实现做一个简单的总结, 主要是loss 层, 别的地方都没啥难度 关于y</description>
    </item>
    
    <item>
      <title>Accelerating Object Detection by Erasing Background Activations 阅读</title>
      <link>/post/omg_net/</link>
      <pubDate>Mon, 10 Feb 2020 13:10:54 +0800</pubDate>
      
      <guid>/post/omg_net/</guid>
      <description>该篇论文来源于Intel, 如其名用来加速目标检测. 主要针对于 one-stage 的目标检测算法. 主要创新点 对于 one-stage 的目标检测算法而言, 由于其设置了大量的 default box, 然后</description>
    </item>
    
    <item>
      <title>Focal loss </title>
      <link>/post/focal_loss/</link>
      <pubDate>Sun, 09 Feb 2020 18:54:07 +0800</pubDate>
      
      <guid>/post/focal_loss/</guid>
      <description>intro 主流的目标检测网络主要包含两种架构,一种是先进行region proposal再分别对得到的region 进行分类与边框回归的 two-stage 网络, RCNN 及其衍</description>
    </item>
    
    <item>
      <title>FPN: Feature Pyramid Networks for Object Detection </title>
      <link>/post/fpn/</link>
      <pubDate>Sun, 09 Feb 2020 14:42:44 +0800</pubDate>
      
      <guid>/post/fpn/</guid>
      <description>常见的检测网络结构 (a) 中使用图像金字塔构建特征金字塔, 在每一个图像尺度下单独提取特征, 耗时 (b) 使用单尺度特征图用来快速的目标检测 (c) 使用单方向多个</description>
    </item>
    
    <item>
      <title>目标检测与实例分割: Mask rcnn</title>
      <link>/post/mask_rcnn/</link>
      <pubDate>Fri, 07 Feb 2020 00:12:19 +0800</pubDate>
      
      <guid>/post/mask_rcnn/</guid>
      <description>mask rcnn 是何凯明团队在faster rcnn的基础上, 将目标检测与实例分割整合到一起的又一力作, 同时改进了faster RCNN 中 ROI pooling存在的 misalignment</description>
    </item>
    
    <item>
      <title>Progan: PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION</title>
      <link>/post/progan%E9%98%85%E8%AF%BB/</link>
      <pubDate>Mon, 20 Jan 2020 22:09:52 +0800</pubDate>
      
      <guid>/post/progan%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 GAN 高清图像生成难的问题 方法 提出新的 GAN 训练方法: 逐渐增加生成器和鉴别器&amp;ndash;从低分辨率开始, 逐渐添加新层，以随着训练的进行网络</description>
    </item>
    
    <item>
      <title>Densecrf与图像分割</title>
      <link>/post/densecrf%E5%B0%8F%E8%AE%A1/</link>
      <pubDate>Thu, 02 Jan 2020 23:54:19 +0800</pubDate>
      
      <guid>/post/densecrf%E5%B0%8F%E8%AE%A1/</guid>
      <description>在图像分割中，在FCN之前，流行的是以概率图模型为代表的传统方法. FCN出来之后一段时间，仍然流行的是以FCN为前端，CRF为后端优化. 随着</description>
    </item>
    
    <item>
      <title>图像修复2: Generative Image Inpainting with Contextual Attention</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</link>
      <pubDate>Thu, 02 Jan 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</guid>
      <description>之前介绍过NVIDIA的图像修复方法, 使用的img-img的方法. 由于GAN网络在图像生成方向的大放异彩, 今天review一下, Adobe的</description>
    </item>
    
    <item>
      <title>图像修复1: Image Inpainting for Irregular Holes Using Partial Convolutions</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</link>
      <pubDate>Sat, 28 Dec 2019 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</guid>
      <description>图像修复一直是CV领域的重点与难点, 基于深度学习的图像修复因为其可以学习到丰富的语义信息和潜在丰富的表达能力, 受到研究者们的热捧. 这篇文章主</description>
    </item>
    
    <item>
      <title>可分离卷积</title>
      <link>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Tue, 24 Dec 2019 22:09:22 +0800</pubDate>
      
      <guid>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</guid>
      <description>可分离卷积主要包括空间可分离卷积（Spatial Separable Convolutions）、深度可分离卷积（Depthwise Separable Convolutions.</description>
    </item>
    
    <item>
      <title>人脸检测网络: SSH: Single Stage Headless Face Detector</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Wed, 18 Dec 2019 22:54:51 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</guid>
      <description>最近看了一下人脸检测的论文, 除了通用的目标检测方法, 看见了这篇论文, 整体上而言和yolo-v3结构是类似的, SSH 设计了不同的检测头. SSH 网络 SSH 网</description>
    </item>
    
    <item>
      <title>Squeeze-and-Excitation Networks</title>
      <link>/post/se-net/</link>
      <pubDate>Wed, 18 Dec 2019 11:59:07 +0800</pubDate>
      
      <guid>/post/se-net/</guid>
      <description>Squeeze-and-Excitation Networks（SENet)是CVPR2018公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v1 实现关键点讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 10 Dec 2019 16:20:31 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>yolo-v1作为anchor free 的目标检测方法, 虽然已经较老，但深入理解其原理还是很有必要的. 对于个人而言, 完全从头实现目标检测算法是必不可</description>
    </item>
    
    <item>
      <title>Facenet: A Unified Embedding for Face Recognition and Clustering</title>
      <link>/post/facenet/</link>
      <pubDate>Thu, 28 Nov 2019 22:31:58 +0800</pubDate>
      
      <guid>/post/facenet/</guid>
      <description>主要创新点 FaceNet 将 face verification(判断是否是同一个人)，recognition(判断是何人)和clustering(寻找相似人脸) 任</description>
    </item>
    
    <item>
      <title>Dcgan: 深度卷积生成对抗网络</title>
      <link>/post/dcgan/</link>
      <pubDate>Tue, 26 Nov 2019 19:58:17 +0800</pubDate>
      
      <guid>/post/dcgan/</guid>
      <description>DCGAN 在 Ian Goodfellow 提出GAN 以来, 在图像领域GAN 可谓被玩的声名大噪. DCAGN 主要是针对卷积实现GAN时, 提出一系列架构设计规则, 使其训练更稳定. 主要有以下</description>
    </item>
    
    <item>
      <title>目标检测之RFB模块: Receptive Field Block Net for Accurate and Fast Object Detection</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Mon, 18 Nov 2019 22:02:39 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</guid>
      <description>RFB 模块主要是针对在保持轻量级网络的速度快、计算量小的情况下, 提升检测的精度, 模块如其名, 从感受野角度入手, 增强轻量级网络的特征表示, 主要用来</description>
    </item>
    
    <item>
      <title>Anchor free 目标检测修炼之路: DenseBox : Unifying Landmark Localization with End to End Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</link>
      <pubDate>Wed, 13 Nov 2019 22:16:21 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</guid>
      <description>DenseBox 是与yolo, faster rcnn同期的目标检测网络, 与yolo v1一样采用 anchor-free的思想, 网络结构采用FCN来实现目标检测 主要创新点</description>
    </item>
    
    <item>
      <title>Yolo: v1-v3</title>
      <link>/post/yolo/</link>
      <pubDate>Sat, 09 Nov 2019 17:16:26 +0800</pubDate>
      
      <guid>/post/yolo/</guid>
      <description>目标检测主要有两种实现，一是faster-rcnn为代表的proposal two-stage 系列，二是以YOLO为代表的one-stage 的回归网络. 主要区</description>
    </item>
    
    <item>
      <title>动手搭建神经网络之:简单联合分割、检测网络</title>
      <link>/post/how_to_built_a_simple_maskrcnn/</link>
      <pubDate>Mon, 28 Oct 2019 23:58:27 +0800</pubDate>
      
      <guid>/post/how_to_built_a_simple_maskrcnn/</guid>
      <description>coursera deeplearning.ai目标检测课后实践，构建一个简化版单目标yolo目标检测并添加前景对象分割分支 网络结构 MASK-Rcnn主要是</description>
    </item>
    
    <item>
      <title>Inception系列</title>
      <link>/post/inception_v1_to_v4/</link>
      <pubDate>Thu, 24 Oct 2019 16:51:27 +0800</pubDate>
      
      <guid>/post/inception_v1_to_v4/</guid>
      <description>对卷积神经网络而言，提升网络的深度与宽度能够显著提升网络的性能，但是网络越大意味着参数量的增加，会使网络更加容易过拟合。同时，增加网络的大小</description>
    </item>
    
    <item>
      <title>Object detection(4): Faster R-CNN</title>
      <link>/post/faster_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sun, 20 Oct 2019 22:10:11 +0800</pubDate>
      
      <guid>/post/faster_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 提出 RPN 网络, 将region proposal 和目标检测统一到一个卷积网络中 anchor 的使用 网络结构 faster rcnn 检测流程如上图所示: 图像经过卷积层提取 feature maps 然后在RPN</description>
    </item>
    
    <item>
      <title>SSD: Single Shot MultiBox Detector</title>
      <link>/post/ssd/</link>
      <pubDate>Fri, 18 Oct 2019 12:19:25 +0800</pubDate>
      
      <guid>/post/ssd/</guid>
      <description>SSD发表在2016ECCV, 是one-stage目标检测算法中经典的框架之一. 其精度优于yolo-v1, 在yolo-v2之后被超越. SSD 具有</description>
    </item>
    
    <item>
      <title>object detection(3): Fast rcnn</title>
      <link>/post/fast_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Wed, 09 Oct 2019 21:24:19 +0800</pubDate>
      
      <guid>/post/fast_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 RCNN, SPPnet的训练是 multi-stage的, 需要每一步训练一个模型. faste rcnn 通过 multi-task loss 将分类与边框回归融合到网络中, 前两者为训练SVM</description>
    </item>
    
    <item>
      <title>Object detection(2): SPPnet</title>
      <link>/post/spp_net/</link>
      <pubDate>Sat, 05 Oct 2019 19:40:34 +0800</pubDate>
      
      <guid>/post/spp_net/</guid>
      <description>在 RCNN 中说到, RCNN存在的一个问题是需要将region proposal进行warp到固定尺寸,这会带来失真等影响. 这是由于具有全连接的CNN</description>
    </item>
    
    <item>
      <title>object detection(1): Rcnn</title>
      <link>/post/rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Thu, 03 Oct 2019 21:24:12 +0800</pubDate>
      
      <guid>/post/rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>Rcnn 作为使用CNN进行目标检测的开山之作, 之后，在其基础上延展出了fast rcnn, faster rcnn, mask rcnn, 等等, 都是在针对前人的问题不断改进, 本文对rcnn 进行小结</description>
    </item>
    
    <item>
      <title>Anchor box</title>
      <link>/post/anchor_box/</link>
      <pubDate>Tue, 01 Oct 2019 15:17:30 +0800</pubDate>
      
      <guid>/post/anchor_box/</guid>
      <description>对于主流的 one-stage(Faster r-cnn&amp;hellip;) 或者是 tow-stage(SSD, YOLO&amp;hellip;)的目标检测算法, 大多都采用了 anchor/prior box机制. Anchor-box 的意义 在yolo-v1 中, 每个grid 输出两个b</description>
    </item>
    
    <item>
      <title>Edge_gan总结</title>
      <link>/post/edge_gan/</link>
      <pubDate>Thu, 19 Sep 2019 23:32:10 +0800</pubDate>
      
      <guid>/post/edge_gan/</guid>
      <description>最近刚好在做分割，顺手玩玩用GAN做边缘检测. 本意是想在BSDS轮廓分割数据集上做，同时验证针对样本极不平衡的损失函数挑选问题，简单做个小结</description>
    </item>
    
    <item>
      <title>L1、L2、Smooth_L1损失函数对比</title>
      <link>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 19 Sep 2019 11:07:59 +0800</pubDate>
      
      <guid>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</guid>
      <description>L1、L2以及Smooth_L1损失函数作为目标检测中回归层常用的损失函数，对他们进行一个对比分析. 三者公式对比如下: 分别对三者求导数: L2</description>
    </item>
    
    <item>
      <title>目标检测非极大值抑制: NMS</title>
      <link>/post/nms/</link>
      <pubDate>Thu, 12 Sep 2019 17:28:58 +0800</pubDate>
      
      <guid>/post/nms/</guid>
      <description>非极大值抑制（Non-Maximum Suppression，NMS），即抑制不是极大值的元素，可以理解为局部最大搜索. 在目标检测中, 无论是 one-stage</description>
    </item>
    
    <item>
      <title>CRNN笔记以及数字检测识别实践</title>
      <link>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 08 Sep 2019 16:07:07 +0800</pubDate>
      
      <guid>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</guid>
      <description>主流的OCR识别分为两个部分:先检测出文字区域再识别文字。检测可采用通用的目标检测方法以及针对于文本检测的网络,识别主要是CRNN及其变体。</description>
    </item>
    
    <item>
      <title>East:An Efficient and Accurate Scene Text Detector阅读及应用</title>
      <link>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 28 Aug 2019 00:03:07 +0800</pubDate>
      
      <guid>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</guid>
      <description>East是旷视科技2017年发表的论文,针对于场景文本检测。East网络也可以轻易的扩展到其他目标检测任务上。我主要在改进版的East基础上</description>
    </item>
    
    <item>
      <title>CycleGAN论文阅读总结及实现</title>
      <link>/post/cyclegan/</link>
      <pubDate>Sat, 24 Aug 2019 21:42:13 +0800</pubDate>
      
      <guid>/post/cyclegan/</guid>
      <description>在cyclegan之前，对于两个域的图像进行转化，比如图像风格转换，它们的训练集图像都是成对的.而cyclegan则解决了训练图像必须成对的</description>
    </item>
    
    <item>
      <title>基于vgg16的半监督视频单目标分割网络&#43; Dense-crf</title>
      <link>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</link>
      <pubDate>Sat, 24 Aug 2019 00:03:12 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</guid>
      <description>.img-wrap{ border: 1px } img{ float: left; width: 25%; height: 160; } one-shot 半监督视频单目标分割 网络实现 采用keras实现，网络结构如下。 类似于unet，但没有unet那么多的参数。使用de</description>
    </item>
    
    <item>
      <title>模版匹配之相关匹配</title>
      <link>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</link>
      <pubDate>Thu, 20 Dec 2018 15:44:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</guid>
      <description>模板匹配 最近准备把学过的一些知识整理写成博客，加深印象。 模板匹配是一种最原始、最基本的模式识别方法，研究某一特定对象物的图案位于图像的什么地</description>
    </item>
    
    <item>
      <title>基于Keras图像相似度计算孪生网络</title>
      <link>/post/keras_simi/</link>
      <pubDate>Mon, 12 Nov 2018 16:32:32 +0800</pubDate>
      
      <guid>/post/keras_simi/</guid>
      <description>import keras from keras.layers import Input,Dense,Conv2D from keras.layers import MaxPooling2D,Flatten,Convolution2D from keras.models import Model import os import numpy as np from PIL import Image from keras.optimizers import SGD from scipy import misc root_path = os.getcwd() train_names = [&#39;bear&#39;,&#39;blackswan&#39;,&#39;bus&#39;,&#39;camel&#39;,&#39;car&#39;,&#39;cows&#39;,&#39;dance&#39;,&#39;dog&#39;,&#39;hike&#39;,&#39;hoc&#39;,&#39;kite&#39;,&#39;lucia&#39;,&#39;mallerd&#39;,&#39;pigs&#39;,&#39;soapbox&#39;,&#39;stro&#39;,&#39;surf&#39;,&#39;swing&#39;,&#39;train&#39;,&#39;walking&#39;] test_names = [&#39;boat&#39;,&#39;dance-jump&#39;,&#39;drift-turn&#39;,&#39;elephant&#39;,&#39;libby&#39;] def load_data(seq_names,data_number,seq_len): #生成图片对 print(&#39;loading data.....&#39;) frame_num = 51 train_data1 = [] train_data2 = [] train_lab = [] count = 0 while count &amp;lt; data_number:</description>
    </item>
    
    <item>
      <title>图像相似度之PSNR与SSIM小结</title>
      <link>/post/psnr/</link>
      <pubDate>Wed, 03 Oct 2018 16:55:08 +0800</pubDate>
      
      <guid>/post/psnr/</guid>
      <description>PSNR（Peak Signal to Noise Ratio）：峰值信噪比 使用局部均值误差来判断差异，对于两个H*W*C的图像，I1,I2 其中n为采样值的比特数，比如</description>
    </item>
    
  </channel>
</rss>