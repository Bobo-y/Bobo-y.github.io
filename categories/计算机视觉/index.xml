<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>计算机视觉 on fly away, chase dream</title>
    <link>/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</link>
    <description>Recent content in 计算机视觉 on fly away, chase dream</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 28 Apr 2020 23:37:42 +0800</lastBuildDate>
    
	<atom:link href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>YOLOv4: Optimal Speed and Accuracy of Object Detection论文解读</title>
      <link>/post/yolov4/</link>
      <pubDate>Tue, 28 Apr 2020 23:37:42 +0800</pubDate>
      
      <guid>/post/yolov4/</guid>
      <description>最近目标检测又出了yolo-v4，作为一个做目标检测的不可不膜拜膜拜。首先由于约瑟夫大神已经退出CV，yolo-v4 的一作是DarkNet的</description>
    </item>
    
    <item>
      <title>目标检测IOU评价指标汇总: GIOU, DIOU, CIOU</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</link>
      <pubDate>Fri, 24 Apr 2020 23:18:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Biou%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%E6%B1%87%E6%80%BB/</guid>
      <description>在目标检测中, IOU 可以被用来评估预测框的性能，IOU越大预测框越准。IOU可表示两个框的距离，IOU越大距离越小. 对于目标检测坐标损失虽然一般</description>
    </item>
    
    <item>
      <title>谷歌最新目标检测论文: EfficientDet</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</link>
      <pubDate>Sat, 18 Apr 2020 11:55:48 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Beffcientdet/</guid>
      <description>最近谷歌放出了 EfficientDet 论文与代码, 在COCO上取得了最好的MAP, 本文对 efficientDet 做个简要的总结, 同时对efficientNet也做个回顾. Efficie</description>
    </item>
    
    <item>
      <title>基于内容的图像检索: pytorch</title>
      <link>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</link>
      <pubDate>Fri, 17 Apr 2020 16:33:21 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84cbir_demo/</guid>
      <description>看了Jason Brownlee博士的Keras CBIR demo, 自己也动手用pytorch写一个. CBIR CBIR 为基于内容的图像检索. 用于在图像数据数据库上检索具有</description>
    </item>
    
    <item>
      <title>超分辨率重建: SRGAN</title>
      <link>/post/srgan/</link>
      <pubDate>Sun, 12 Apr 2020 19:41:57 +0800</pubDate>
      
      <guid>/post/srgan/</guid>
      <description>对于图像超分辨率重建, 第一个使用CNN实现的是SRCNN, 类似于编码器解码器结构. SRGAN是第一个使用GAN网络解决超分辨率重构的网络 创新</description>
    </item>
    
    <item>
      <title>语义分割: UNET</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:46 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2unet/</guid>
      <description>说到语义分割, 不得不说一下U-net, U-net首先针对于医学图像分割提出, 由于其卓越的性能, 目前大部分医学图像分割都是基于U-net或者U</description>
    </item>
    
    <item>
      <title>语义分割: deeplab V1到deeplab V3</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:32 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2deeplab-v1%E5%88%B0deeplab-v3/</guid>
      <description>deeplab 为一个系列, 因此将其放在一起进行个回顾 Deeplab-v1 与deeplab-v2 将deeplab-v1与deeplab-v2放在一起, 主要是因为二者总体结构</description>
    </item>
    
    <item>
      <title>语义分割之: FCN</title>
      <link>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</link>
      <pubDate>Fri, 10 Apr 2020 23:11:05 +0800</pubDate>
      
      <guid>/post/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2fcn/</guid>
      <description>研究生阶段自己对分割这边还是很熟悉的, 工作后发现很多网络只能说出原理和整体框架, 面试时问的很细节, 再次将经典分割网络仔细review一遍. 主</description>
    </item>
    
    <item>
      <title>人脸检测网络: MTCNN</title>
      <link>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</link>
      <pubDate>Sat, 04 Apr 2020 18:03:03 +0800</pubDate>
      
      <guid>/post/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9Cmtcnn/</guid>
      <description>之前做人脸检测使用的是retinaface做的, 刚好最近被问到MTCNN, 以前没有细看, 正好做个笔记. MTCNN是2015年提出的用于人脸检</description>
    </item>
    
    <item>
      <title>Anchor free目标检测修炼之路:fcos -Fully Convolutional One-Stage Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</link>
      <pubDate>Tue, 10 Mar 2020 23:35:58 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFfcos/</guid>
      <description>主流的目标检测算法大多数是基于anchor box的, one-stage 的yolo-v2, yolo-v3, ssd&amp;hellip;以及two-stage 的faster rcnn</description>
    </item>
    
    <item>
      <title>图像修复3: Free-Form Image Inpainting with Gated Convolution</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</link>
      <pubDate>Thu, 20 Feb 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D3gated_cobv/</guid>
      <description>此文与图像修复2中review过的论文一样, 出至Adobe同一人之手. 主要创新点 提出门控卷积解决普通卷积将所有输入像素都视为有效像素的问题,</description>
    </item>
    
    <item>
      <title>OHEM: 在线困难样本挖掘: Training Region-based Object Detectors with Online Hard Example Mining</title>
      <link>/post/ohem/</link>
      <pubDate>Sun, 16 Feb 2020 18:46:23 +0800</pubDate>
      
      <guid>/post/ohem/</guid>
      <description>在目标检测中, 存在正负样本类别不平衡的现象, 特别是对于单阶段的目标检测算法. 如果每张训练图片中目标个数还很少的话, 背景区域就占了绝大部分, 分</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v2 损失函数实现讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 10 Feb 2020 16:20:36 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v2%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>最近闲暇时自己在pytorch实现并训练了yolo-v2, 对yolo-v2的实现做一个简单的总结, 主要是loss 层, 别的地方都没啥难度 关于y</description>
    </item>
    
    <item>
      <title>Accelerating Object Detection by Erasing Background Activations 阅读</title>
      <link>/post/omg_net/</link>
      <pubDate>Mon, 10 Feb 2020 13:10:54 +0800</pubDate>
      
      <guid>/post/omg_net/</guid>
      <description>该篇论文来源于Intel, 如其名用来加速目标检测. 主要针对于 one-stage 的目标检测算法. 主要创新点 对于 one-stage 的目标检测算法而言, 由于其设置了大量的 default box, 然后</description>
    </item>
    
    <item>
      <title>Focal loss </title>
      <link>/post/focal_loss/</link>
      <pubDate>Sun, 09 Feb 2020 18:54:07 +0800</pubDate>
      
      <guid>/post/focal_loss/</guid>
      <description>intro 主流的目标检测网络主要包含两种架构,一种是先进行region proposal再分别对得到的region 进行分类与边框回归的 two-stage 网络, RCNN 及其衍</description>
    </item>
    
    <item>
      <title>FPN: Feature Pyramid Networks for Object Detection </title>
      <link>/post/fpn/</link>
      <pubDate>Sun, 09 Feb 2020 14:42:44 +0800</pubDate>
      
      <guid>/post/fpn/</guid>
      <description>常见的检测网络结构 (a) 中使用图像金字塔构建特征金字塔, 在每一个图像尺度下单独提取特征, 耗时 (b) 使用单尺度特征图用来快速的目标检测 (c) 使用单方向多个</description>
    </item>
    
    <item>
      <title>目标检测与实例分割: Mask rcnn</title>
      <link>/post/mask_rcnn/</link>
      <pubDate>Fri, 07 Feb 2020 00:12:19 +0800</pubDate>
      
      <guid>/post/mask_rcnn/</guid>
      <description>mask rcnn 是何凯明团队在faster rcnn的基础上, 将目标检测与实例分割整合到一起的又一力作, 同时改进了faster RCNN 中 ROI pooling存在的 misalignment</description>
    </item>
    
    <item>
      <title>Progan: PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION</title>
      <link>/post/progan%E9%98%85%E8%AF%BB/</link>
      <pubDate>Mon, 20 Jan 2020 22:09:52 +0800</pubDate>
      
      <guid>/post/progan%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 GAN 高清图像生成难的问题 方法 提出新的 GAN 训练方法: 逐渐增加生成器和鉴别器&amp;ndash;从低分辨率开始, 逐渐添加新层，以随着训练的进行网络</description>
    </item>
    
    <item>
      <title>Densecrf与图像分割</title>
      <link>/post/densecrf%E5%B0%8F%E8%AE%A1/</link>
      <pubDate>Thu, 02 Jan 2020 23:54:19 +0800</pubDate>
      
      <guid>/post/densecrf%E5%B0%8F%E8%AE%A1/</guid>
      <description>在图像分割中，在FCN之前，流行的是以概率图模型为代表的传统方法. FCN出来之后一段时间，仍然流行的是以FCN为前端，CRF为后端优化. 随着</description>
    </item>
    
    <item>
      <title>图像修复2: Generative Image Inpainting with Contextual Attention</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</link>
      <pubDate>Thu, 02 Jan 2020 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2%E4%B8%8A%E4%B8%8B%E6%96%87attention/</guid>
      <description>之前介绍过NVIDIA的图像修复方法, 使用的img-img的方法. 由于GAN网络在图像生成方向的大放异彩, 今天review一下, Adobe的</description>
    </item>
    
    <item>
      <title>图像修复1: Image Inpainting for Irregular Holes Using Partial Convolutions</title>
      <link>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</link>
      <pubDate>Sat, 28 Dec 2019 01:00:09 +0800</pubDate>
      
      <guid>/post/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1partial_conv/</guid>
      <description>图像修复一直是CV领域的重点与难点, 基于深度学习的图像修复因为其可以学习到丰富的语义信息和潜在丰富的表达能力, 受到研究者们的热捧. 这篇文章主</description>
    </item>
    
    <item>
      <title>可分离卷积</title>
      <link>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Tue, 24 Dec 2019 22:09:22 +0800</pubDate>
      
      <guid>/post/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/</guid>
      <description>可分离卷积主要包括空间可分离卷积（Spatial Separable Convolutions）、深度可分离卷积（Depthwise Separable Convolutions.</description>
    </item>
    
    <item>
      <title>人脸检测网络: SSH: Single Stage Headless Face Detector</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Wed, 18 Dec 2019 22:54:51 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bssh%E7%BD%91%E7%BB%9C/</guid>
      <description>最近看了一下人脸检测的论文, 除了通用的目标检测方法, 看见了这篇论文, 整体上而言和yolo-v3结构是类似的, SSH 设计了不同的检测头. SSH 网络 SSH 网</description>
    </item>
    
    <item>
      <title>Squeeze-and-Excitation Networks</title>
      <link>/post/se-net/</link>
      <pubDate>Wed, 18 Dec 2019 11:59:07 +0800</pubDate>
      
      <guid>/post/se-net/</guid>
      <description>Squeeze-and-Excitation Networks（SENet)是CVPR2018公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提</description>
    </item>
    
    <item>
      <title>目标检测网络: yolo-v1 实现关键点讲解</title>
      <link>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 10 Dec 2019 16:20:31 +0800</pubDate>
      
      <guid>/post/%E5%A6%82%E4%BD%95%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%9E%84%E5%BB%BAyolo-v1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/</guid>
      <description>yolo-v1作为anchor free 的目标检测方法, 虽然已经较老，但深入理解其原理还是很有必要的. 对于个人而言, 完全从头实现目标检测算法是必不可</description>
    </item>
    
    <item>
      <title>Facenet: A Unified Embedding for Face Recognition and Clustering</title>
      <link>/post/facenet/</link>
      <pubDate>Thu, 28 Nov 2019 22:31:58 +0800</pubDate>
      
      <guid>/post/facenet/</guid>
      <description>主要创新点 FaceNet 将 face verification(判断是否是同一个人)，recognition(判断是何人)和clustering(寻找相似人脸) 任</description>
    </item>
    
    <item>
      <title>Dcgan: 深度卷积生成对抗网络</title>
      <link>/post/dcgan/</link>
      <pubDate>Tue, 26 Nov 2019 19:58:17 +0800</pubDate>
      
      <guid>/post/dcgan/</guid>
      <description>DCGAN 在 Ian Goodfellow 提出GAN 以来, 在图像领域GAN 可谓被玩的声名大噪. DCAGN 主要是针对卷积实现GAN时, 提出一系列架构设计规则, 使其训练更稳定. 主要有以下</description>
    </item>
    
    <item>
      <title>目标检测之RFB模块: Receptive Field Block Net for Accurate and Fast Object Detection</title>
      <link>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</link>
      <pubDate>Mon, 18 Nov 2019 22:02:39 +0800</pubDate>
      
      <guid>/post/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8Brfb%E6%A8%A1%E5%9D%97/</guid>
      <description>RFB 模块主要是针对在保持轻量级网络的速度快、计算量小的情况下, 提升检测的精度, 模块如其名, 从感受野角度入手, 增强轻量级网络的特征表示, 主要用来</description>
    </item>
    
    <item>
      <title>Anchor free 目标检测修炼之路: DenseBox : Unifying Landmark Localization with End to End Object Detection</title>
      <link>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</link>
      <pubDate>Wed, 13 Nov 2019 22:16:21 +0800</pubDate>
      
      <guid>/post/anchor_free%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AFdense_box/</guid>
      <description>DenseBox 是与yolo, faster rcnn同期的目标检测网络, 与yolo v1一样采用 anchor-free的思想, 网络结构采用FCN来实现目标检测 主要创新点</description>
    </item>
    
    <item>
      <title>Yolo: v1-v3</title>
      <link>/post/yolo/</link>
      <pubDate>Sat, 09 Nov 2019 17:16:26 +0800</pubDate>
      
      <guid>/post/yolo/</guid>
      <description>目标检测主要有两种实现，一是faster-rcnn为代表的proposal two-stage 系列，二是以YOLO为代表的one-stage 的回归网络. 主要区</description>
    </item>
    
    <item>
      <title>动手搭建神经网络之:简单联合分割、检测网络</title>
      <link>/post/how_to_built_a_simple_maskrcnn/</link>
      <pubDate>Mon, 28 Oct 2019 23:58:27 +0800</pubDate>
      
      <guid>/post/how_to_built_a_simple_maskrcnn/</guid>
      <description>coursera deeplearning.ai目标检测课后实践，构建一个简化版单目标yolo目标检测并添加前景对象分割分支 网络结构 MASK-Rcnn主要是</description>
    </item>
    
    <item>
      <title>Inception系列</title>
      <link>/post/inception_v1_to_v4/</link>
      <pubDate>Thu, 24 Oct 2019 16:51:27 +0800</pubDate>
      
      <guid>/post/inception_v1_to_v4/</guid>
      <description>对卷积神经网络而言，提升网络的深度与宽度能够显著提升网络的性能，但是网络越大意味着参数量的增加，会使网络更加容易过拟合。同时，增加网络的大小</description>
    </item>
    
    <item>
      <title>Object detection(4): Faster R-CNN</title>
      <link>/post/faster_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sun, 20 Oct 2019 22:10:11 +0800</pubDate>
      
      <guid>/post/faster_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 提出 RPN 网络, 将region proposal 和目标检测统一到一个卷积网络中 anchor 的使用 网络结构 faster rcnn 检测流程如上图所示: 图像经过卷积层提取 feature maps 然后在RPN</description>
    </item>
    
    <item>
      <title>SSD: Single Shot MultiBox Detector</title>
      <link>/post/ssd/</link>
      <pubDate>Fri, 18 Oct 2019 12:19:25 +0800</pubDate>
      
      <guid>/post/ssd/</guid>
      <description>SSD发表在2016ECCV, 是one-stage目标检测算法中经典的框架之一. 其精度优于yolo-v1, 在yolo-v2之后被超越. SSD 具有</description>
    </item>
    
    <item>
      <title>object detection(3): Fast rcnn</title>
      <link>/post/fast_rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Wed, 09 Oct 2019 21:24:19 +0800</pubDate>
      
      <guid>/post/fast_rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>主要贡献 RCNN, SPPnet的训练是 multi-stage的, 需要每一步训练一个模型. faste rcnn 通过 multi-task loss 将分类与边框回归融合到网络中, 前两者为训练SVM</description>
    </item>
    
    <item>
      <title>Object detection(2): SPPnet</title>
      <link>/post/spp_net/</link>
      <pubDate>Sat, 05 Oct 2019 19:40:34 +0800</pubDate>
      
      <guid>/post/spp_net/</guid>
      <description>在 RCNN 中说到, RCNN存在的一个问题是需要将region proposal进行warp到固定尺寸,这会带来失真等影响. 这是由于具有全连接的CNN</description>
    </item>
    
    <item>
      <title>object detection(1): Rcnn</title>
      <link>/post/rcnn%E9%98%85%E8%AF%BB/</link>
      <pubDate>Thu, 03 Oct 2019 21:24:12 +0800</pubDate>
      
      <guid>/post/rcnn%E9%98%85%E8%AF%BB/</guid>
      <description>Rcnn 作为使用CNN进行目标检测的开山之作, 之后，在其基础上延展出了fast rcnn, faster rcnn, mask rcnn, 等等, 都是在针对前人的问题不断改进, 本文对rcnn 进行小结</description>
    </item>
    
    <item>
      <title>Anchor box</title>
      <link>/post/anchor_box/</link>
      <pubDate>Tue, 01 Oct 2019 15:17:30 +0800</pubDate>
      
      <guid>/post/anchor_box/</guid>
      <description>对于主流的 one-stage(Faster r-cnn&amp;hellip;) 或者是 tow-stage(SSD, YOLO&amp;hellip;)的目标检测算法, 大多都采用了 anchor/prior box机制. Anchor-box 的意义 在yolo-v1 中, 每个grid 输出两个b</description>
    </item>
    
    <item>
      <title>Edge_gan总结</title>
      <link>/post/edge_gan/</link>
      <pubDate>Thu, 19 Sep 2019 23:32:10 +0800</pubDate>
      
      <guid>/post/edge_gan/</guid>
      <description>最近刚好在做分割，顺手玩玩用GAN做边缘检测. 本意是想在BSDS轮廓分割数据集上做，同时验证针对样本极不平衡的损失函数挑选问题，简单做个小结</description>
    </item>
    
    <item>
      <title>L1、L2、Smooth_L1损失函数对比</title>
      <link>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 19 Sep 2019 11:07:59 +0800</pubDate>
      
      <guid>/post/l1_l2_smoothl1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94/</guid>
      <description>L1、L2以及Smooth_L1损失函数作为目标检测中回归层常用的损失函数，对他们进行一个对比分析. 三者公式对比如下: 分别对三者求导数: L2</description>
    </item>
    
    <item>
      <title>目标检测非极大值抑制: NMS</title>
      <link>/post/nms/</link>
      <pubDate>Thu, 12 Sep 2019 17:28:58 +0800</pubDate>
      
      <guid>/post/nms/</guid>
      <description>非极大值抑制（Non-Maximum Suppression，NMS），即抑制不是极大值的元素，可以理解为局部最大搜索. 在目标检测中, 无论是 one-stage</description>
    </item>
    
    <item>
      <title>CRNN笔记以及数字检测识别实践</title>
      <link>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 08 Sep 2019 16:07:07 +0800</pubDate>
      
      <guid>/post/crnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%BB%A5%E5%8F%8A%E6%95%B0%E5%AD%97%E6%A3%80%E6%B5%8B%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/</guid>
      <description>主流的OCR识别分为两个部分:先检测出文字区域再识别文字。检测可采用通用的目标检测方法以及针对于文本检测的网络,识别主要是CRNN及其变体。</description>
    </item>
    
    <item>
      <title>East:An Efficient and Accurate Scene Text Detector阅读及应用</title>
      <link>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</link>
      <pubDate>Wed, 28 Aug 2019 00:03:07 +0800</pubDate>
      
      <guid>/post/east%E5%8F%8A%E5%BA%94%E7%94%A8/</guid>
      <description>East是旷视科技2017年发表的论文,针对于场景文本检测。East网络也可以轻易的扩展到其他目标检测任务上。我主要在改进版的East基础上</description>
    </item>
    
    <item>
      <title>CycleGAN论文阅读总结及实现</title>
      <link>/post/cyclegan/</link>
      <pubDate>Sat, 24 Aug 2019 21:42:13 +0800</pubDate>
      
      <guid>/post/cyclegan/</guid>
      <description>在cyclegan之前，对于两个域的图像进行转化，比如图像风格转换，它们的训练集图像都是成对的.而cyclegan则解决了训练图像必须成对的</description>
    </item>
    
    <item>
      <title>基于vgg16的半监督视频单目标分割网络&#43; Dense-crf</title>
      <link>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</link>
      <pubDate>Sat, 24 Aug 2019 00:03:12 +0800</pubDate>
      
      <guid>/post/%E5%9F%BA%E4%BA%8Evgg%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/</guid>
      <description>.img-wrap{ border: 1px } img{ float: left; width: 25%; height: 160; } one-shot 半监督视频单目标分割 网络实现 采用keras实现，网络结构如下。 类似于unet，但没有unet那么多的参数。使用de</description>
    </item>
    
    <item>
      <title>模版匹配之相关匹配</title>
      <link>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</link>
      <pubDate>Thu, 20 Dec 2018 15:44:06 +0800</pubDate>
      
      <guid>/post/%E7%9B%B8%E5%85%B3%E5%8C%B9%E9%85%8D/</guid>
      <description>模板匹配 最近准备把学过的一些知识整理写成博客，加深印象。 模板匹配是一种最原始、最基本的模式识别方法，研究某一特定对象物的图案位于图像的什么地</description>
    </item>
    
    <item>
      <title>基于Keras图像相似度计算孪生网络</title>
      <link>/post/keras_simi/</link>
      <pubDate>Mon, 12 Nov 2018 16:32:32 +0800</pubDate>
      
      <guid>/post/keras_simi/</guid>
      <description>import keras from keras.layers import Input,Dense,Conv2D from keras.layers import MaxPooling2D,Flatten,Convolution2D from keras.models import Model import os import numpy as np from PIL import Image from keras.optimizers import SGD from scipy import misc root_path = os.getcwd() train_names = [&#39;bear&#39;,&#39;blackswan&#39;,&#39;bus&#39;,&#39;camel&#39;,&#39;car&#39;,&#39;cows&#39;,&#39;dance&#39;,&#39;dog&#39;,&#39;hike&#39;,&#39;hoc&#39;,&#39;kite&#39;,&#39;lucia&#39;,&#39;mallerd&#39;,&#39;pigs&#39;,&#39;soapbox&#39;,&#39;stro&#39;,&#39;surf&#39;,&#39;swing&#39;,&#39;train&#39;,&#39;walking&#39;] test_names = [&#39;boat&#39;,&#39;dance-jump&#39;,&#39;drift-turn&#39;,&#39;elephant&#39;,&#39;libby&#39;] def load_data(seq_names,data_number,seq_len): #生成图片对 print(&#39;loading data.....&#39;) frame_num = 51 train_data1 = [] train_data2 = [] train_lab = [] count = 0 while count &amp;lt; data_number:</description>
    </item>
    
    <item>
      <title>图像相似度之PSNR与SSIM小结</title>
      <link>/post/psnr/</link>
      <pubDate>Wed, 03 Oct 2018 16:55:08 +0800</pubDate>
      
      <guid>/post/psnr/</guid>
      <description>PSNR（Peak Signal to Noise Ratio）：峰值信噪比 使用局部均值误差来判断差异，对于两个H*W*C的图像，I1,I2 其中n为采样值的比特数，比如</description>
    </item>
    
  </channel>
</rss>