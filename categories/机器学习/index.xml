<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Lin Yang&#39;s Blog</title>
    <link>https://yl305237731.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Lin Yang&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 22 Dec 2018 15:34:07 +0800</lastBuildDate>
    
	<atom:link href="https://yl305237731.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>监督分类之：KNN算法</title>
      <link>https://yl305237731.github.io/post/knn/</link>
      <pubDate>Sat, 22 Dec 2018 15:34:07 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/knn/</guid>
      <description>KNN简介 K近邻（K-Nearest Neighbor）学习是一种简单的监督学习方法。方法流程主要是：对于给定的测试样本，基于某种距离度量找出训练集中与其最靠近的K个样本，根据这K个样本的类别来决定测试样本的类别，一般采用投票法，即判别为K个中出现次数最多的类别。（这种思路很早就知道了，完全不觉得这也属于高大上的机器学习啊，明明没有‘’学习‘’啊）。 所以，KNN是‘懒惰学习’的代表：无训练时间开销。相对的，训练阶段进行学习处理的为‘急切学习’，显然，大部分的机器学习方法都属于后者，比如：SVM,DNN,LR等。
KNN示例 假设训练样本为两类二维样本，红色的 +，和绿色的-。蓝色的点为测试样本 KNN分类需要定义相似性度量函数，对于本例二维点可以采用欧氏距离函数作为判别，计算测试点到所有样本点的距离，选取前K个判断类别。
对于KNN，找到合适的距离度量函数很重要，同时，K值的选取对结果也起着很大的作用。
当K小时，只有最靠近输入实例的训练实例对预测结果起作用，也容易过拟合，导致高方差； 当K很大时，与输入实例不相似的训练实例对训练结果也起作用，会导致高的偏差
总体流程如下： 1. 计算训练集中的每个样本与测试样本的距离 2. 将距离按地址排序 3. 选取与当前测试样本距离最小的K个样本 4. 计算K个样本所属类别出现的频率 5. 选取频率最高点的所属类别为预测分类
基于KNN的mnist数据集分类 mnist手写数字数据集包含（0-9）10个类别的手写数字 ，训练集有55000张图片，测试集有1万张图片，可视化部分结果如下： 使用的图片数据是将图片转换为行向量之后的数据（数据来源TensorFlow里tutorials.mnist里的数据），数据已进行归一化(即像素值/255)。
TensorFlow环境下代码
from tensorflow.examples.tutorials.mnist import input_data import numpy as np import time class Knn(): def classMnist(self,sample,train_data,train_label,k = 10): dataRows = np.shape(train_data)[0] sampleRep = np.tile(sample,(dataRows,1)) # 将测试样本变为和训练集一样大小的数组，不用循环计算 distance = np.sqrt(np.sum(np.power(sampleRep - train_data,2),axis=1)) # 采用欧式距离 sortIndex = np.argsort(distance) countClass = {} for i in range(k): label = train_label[sortIndex[i]][0] if str(label) in countClass: countClass[str(label)] = countClass.</description>
    </item>
    
    <item>
      <title>线性拟合笔记之：Ransac算法</title>
      <link>https://yl305237731.github.io/post/ransac/</link>
      <pubDate>Wed, 19 Dec 2018 15:52:22 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/ransac/</guid>
      <description>关于Ransac算法 RANSAC为Random Sample Consensus，即随机采样一致性算法，是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。在计算机视觉中用的比较多，如特征点匹配。本文主要从线性拟合角度分析。
Ransac算法 有样本数据集如上图所示，其中蓝色为正确样本，绿色和红色为噪声样本，我们想要拟合一个线性模型，如果使用最小二乘法的话，结果如下图： 对于这个数据样本而言，由于噪声偏离正确数据不是太远且噪声少，拟合结果偏差不是太大。但是当噪声比例或偏离很大时，基于全局的最小二乘法几乎无法得到好的结果。
RANSAC算法的基本假设是样本中包含正确数据(inliers，可以被模型描述的数据)，也包含异常数据(outliers，偏离正常范围很远、无法适应数学模型的数据)，即数据集中含有噪声。 主要思想是通过不断的从样本中随机选择一定的样本来拟合模型，然后用未被选中的样本测试模型，根据一定的规则保留最优模型。
算法流程如下 一：随机选择n个样本，作为inliers； 二：用inliers拟合一个模型（本文做线性拟合，采用最小二乘），然后用模型测试outliers，如果某个outliers与模型的误差小于给定的阈值，将其加入inliers； 三：如果inliers中样本个数大于设定的值，得到认为正确的模型。然后用新的inliers重新估计模型； 四：执行以上过程指定的轮数，每次产生的模型要么因为inliers太少而被舍弃，要么比现有的模型更好而被选中。
Ransac线性拟合实验 部分中间迭代结果：
最佳拟合： Python代码 import matplotlib.pyplot as plt from numpy import * import numpy as np import operator as op class Ransac: weight = 0. bias = 0. def least_square(self,samples): ##最小二乘法 x = samples[:,0] y = samples[:,1] x_ = 0 y_ = 0 x_mul_y = 0 x_2 = 0 n = len(x) for i in range(n): x_ = x[i] + x_ y_ = y[i] + y_ x_mul_y = x[i] * y[i] + x_mul_y x_2 = x[i] * x[i] + x_2 x_ = x_ / n y_ = y_ / n weight = (x_mul_y - n * x_ * y_) / (x_2 - n * x_ * x_) bias = y_ - weight * x_ return weight,bias def isRepeat(self,sour,tar): #判断是否含有重复样本 for i in range(len(sour)): if (op.</description>
    </item>
    
    <item>
      <title>线性拟合笔记之：最小二乘法</title>
      <link>https://yl305237731.github.io/post/linersqur/</link>
      <pubDate>Tue, 18 Dec 2018 15:57:45 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/linersqur/</guid>
      <description>关于最小二乘法 以下是百度百科的解释：最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。 对于线性拟合，给定的具有线性关系的观测样本，可以通过最小二乘法求解得到线性模型。
一元线性回归模型求解 给定的观测数据 ：(x1,y1),(x2,y1),(x3,y3)&amp;hellip;&amp;hellip;(xn,yn)，假设其存在着线性关系，线性模型为：$$ \bar{y} = wx +b $$ 优化目标函数为： 即标签值和预测值之间的均方误差，所选择的回归模型应该使所有观察值的残差平方和达到最小， 也就是 Least square。 将目标函数展开： 此时，L是关于w 和 b 的函数，要使 L的值最小，由数学知识可知，可以通过求偏导令为0可解得，即： 记: 则 同理可得： 即可求得w,b。
最小二乘实现 一元最小二乘法 由上面的 w 可以看出，整个计算结果依赖于$\bar{x}$,$\bar{y}$,$x{i}y{i}$,$x_{i}^{2}$
python测试代码如下：
import matplotlib.pyplot as plt import numpy as np class Least_square: weight = 0 bias = 0 def get_weight(self): return self.weight def get_bias(self): return self.bias def least_square(self,x,y): if len(x)!= len(y): print(&#39;data error&#39;) return x_ = 0 y_ = 0 x_mul_y = 0 x_2 = 0 n = len(x) for i in range(n): x_ = x[i] + x_ y_ = y[i] + y_ x_mul_y = x[i]*y[i] + x_mul_y x_2 = x[i]*x[i] + x_2 x_ = x_ / n y_ = y_ / n self.</description>
    </item>
    
    <item>
      <title>Ubuntu &#43; Python下libsvm使用小结</title>
      <link>https://yl305237731.github.io/post/libsvm/</link>
      <pubDate>Wed, 28 Nov 2018 16:24:49 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/libsvm/</guid>
      <description>关于libsvm libsvm是台湾大学林智仁(Chih-Jen Lin)教授等开发，它主要用于分类(支持二分类和多分类)和回归，主页主页，下载网站是链接，目前更新到3.23版本。
安装测试 我是在Ubuntu16.04环境下使用的 解压后目录如下： 针对Python，进入Python包，内容如下： 测试： 命令行输入：
import sys sys.path.append(&#39;/home/thinkstation/liny/libsvm/python&#39;) import svm  如果系统里是第一次使用libsvm,此时会报错 LIBSVM library not found 进入linsvm目录，命令行输入
make lib / /得到 libsvm.so.2  本地目录下测试可直接使用 加入系统路径： 加Python包下的.py文件放到/usr/lib/python2.7/dist-packages中，libsvm.so.2放到/usr/local/lib/python2.7中
使用 1.使用 a1a二分类数据集 链接 特征已经进行了处理，libsvm 数据格式如下： 可以看出，格式为： 类别 index：特征 index：特征 &amp;hellip;&amp;hellip;.
2.mnist数据集 libsvm官网数据集中的mnist数据 其中前两项数据没有经过scale，后两项没有经过scale 用未scale的数据训练测试： 训练了接近5个小时，-_-||
精度竟然这么低。。。。。 换上scale后的数据： 训练时间10分钟左右，精度0.94
3.猫狗二分类 数据准备：猫狗图片各450张，大小 50*50，按照格式将每张图片转为一行，并将像素值归为0-1 准确率0.62，可能数据有点少吧。。。。 总之，作者写的很完善了，还在不断优化中。 感觉我们如果要使用的话，需要自己做的主要是数据、特征方面的处理等，高深一点的就研究并行
主要使用函数 svmutil.py中 1. svm_load_model(model_file_name)
2. svm_save_model(model_file_name, model)
3. svm_train(arg1, arg2=None, arg3=None)
4. svm_predict(y, x, m, options=&amp;ldquo;&amp;rdquo;)</description>
    </item>
    
  </channel>
</rss>