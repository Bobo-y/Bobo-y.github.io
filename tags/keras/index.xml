<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Keras on Lin Yang&#39;s Blog</title>
    <link>https://yl305237731.github.io/tags/keras/</link>
    <description>Recent content in Keras on Lin Yang&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 21 Jul 2019 21:25:20 +0800</lastBuildDate>
    
	<atom:link href="https://yl305237731.github.io/tags/keras/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>基于yolo_v3的水印检测</title>
      <link>https://yl305237731.github.io/post/yovo-keras/</link>
      <pubDate>Sun, 21 Jul 2019 21:25:20 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/yovo-keras/</guid>
      <description>背景 近年来版权意识的提高，在使用别人图片的时候（尤其是商业领域），需要检测图片是否有别的公司的水印（ 主要针对人眼可见的水印，除去数字加密等水印）。 传统的水印检测方法主要是模版匹配方法，可能包含对 水印图片进行一系列的操作。 缺点很明显，水印种类多，即使同一类水印其大小等特征都具有明显的区别，通过 模版匹配的方法难以有效的检测。
基于推荐的Faster RCNN系列 由于深度学习盛行，计算机视觉任务大多转向了基于深度学习实现。对于基于区域推荐的Faster RCNN系列 目标检测框架，通过two-step的形式进行检测。虽然faster rcnn将区域推荐以RPN实现，但其实时检测能力 仍然较弱。同时faster rcnn 实现较繁琐，自己训练模型往往达不到别人的效果。
一步到位的SSD，YOLO系列 与基于区域推荐的框架不同，YOLO系列是直接在整张输入图片上进行回归，以检测出所有的目标。公开数据集 上的对比结果而言，YOLO系列的速度大大领先与faster rcnn一类，但精度稍差。
基于YOLO_v3的小目标检测 偶然在fastai上看见一篇文章，文章明确了yolo_v3对小目标的检测能力。yolo_v3网络本身在设计时就考虑 到了小目标检测，网络在进行损失计算的时候计算了3个尺度的损失。对大部分水印而言，其实也并不完全是小 目标检测。因此，考虑到速度与精度。选择了yolo_v3. 关于yolo_v3模型的blog太多了，略过。 模型有了，剩下就是数据和调参数了。 在经过大量调参后，最终模型yolo_v3水印检测模型
结果 最终测试集水印召回率可达95%。模型依赖数据，水印模版越多，召回率可更高。
展望 以上实验并未修改模型，时间几乎全花在生成水印图片以及调参上。对于市面上的水印而言，80%的水印出现的位置固定， 个人觉得加入attention机制效果应该还会提升。</description>
    </item>
    
    <item>
      <title>Keras数据增强并保存到本地</title>
      <link>https://yl305237731.github.io/post/keras_data_aug/</link>
      <pubDate>Fri, 29 Mar 2019 15:16:16 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/keras_data_aug/</guid>
      <description>当需要对指定文件夹下的图片进行数据增广时，使用keras的ImageDataGenerator类的flow_from_directory（）方法可快速的实现 1.首先实例化ImageDataGenerator对象以自己想要的属性 img_datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, shear_range=0.0, zoom_range=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, ) #### 对于数据增强主要可能用到的属性如下，按自己的需求设置即可： ##### featurewise_center: 布尔值。将输入数据的均值设置为 0，逐特征进行。 ##### samplewise_center: 布尔值。将每个样本的均值设置为 0。 ##### featurewise_std_normalization: Boolean. 布尔值。将输入除以数据标准差，逐特征进行。 ##### zca_epsilon: ZCA 白化的 epsilon 值，默认为 1e-6。 ##### zca_whitening: 布尔值。是否应用 ZCA 白化。 ##### rotation_range: 整数。随机旋转的度数范围。 ##### width_shift_range: 浮点数、一维数组或整数 ##### height_shift_range: 浮点数、一维数组或整数 ##### shear_range: 浮点数。剪切强度（以弧度逆时针方向剪切角度）。 ##### zoom_range: 浮点数 或 [lower, upper]。随机缩放范围。 ##### channel_shift_range: 浮点数。随机通道转换的范围。 ##### horizontal_flip: 布尔值。随机水平翻转。 ##### vertical_flip: 布尔值。随机垂直翻转。  2.</description>
    </item>
    
    <item>
      <title>Keras多标签分类网络实现</title>
      <link>https://yl305237731.github.io/post/keras_duofenlei/</link>
      <pubDate>Fri, 29 Mar 2019 15:07:11 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/keras_duofenlei/</guid>
      <description>简谈多分类与多标签分类 简单的说，输入一张图片进行分类： * 这张图片里面的物体（通常认为只有一个物体）属于某一个类，各个类别之间的概率是竞争关系，取最高概率标签为物体的类别。所以，多分类最后的激活为softmax函数。 * 实际情况下，一个图片只能有一个物体未免太限制了，能不能一次性判断出图片里面多个物体，比如既有人又有车，网络输出含有每个物体的概率，其概率是非竞争的，这就是多标签分类。
数据准备 我自己做的是一个8标签的分类，当然了一张图片里面最多也就同时包含4个左右的物体
数据目录如下： 按以下格式写的data_train.txt,我是从txt中读取图片路径加载训练图片的 数据生成 我没有使用ImageDataGenerator，使用生成器从txt中加载图像。一共有8各类，标签为1x8的向量，图片中包含某个物体，向量对于位置置1。如一张图中包含id为2，5，7三个物体，标签为[0,1,0,0,1,0,1,0]
data_gen.py
import os import numpy as np from PIL import Image def to_multi_label(num_list, num_class): lab = np.zeros(shape=(1,num_class)) for i in num_list: lab[0,int(i)-1] = 1 return lab def generate_arrays_from_txt(path, batch_size, num_class): with open(path) as f: while True: imgs = [] labs = np.zeros(shape=(batch_size,num_class)) i= 0 while len(imgs) &amp;lt; batch_size: line = f.readline() if not line: f.seek(0) line = f.</description>
    </item>
    
    <item>
      <title>Keras数据集加载小结</title>
      <link>https://yl305237731.github.io/post/keras_dataload/</link>
      <pubDate>Tue, 26 Mar 2019 15:20:23 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/keras_dataload/</guid>
      <description>对于keras加载训练数据，官方上没有详说。然而网上查各种资料，写法太多，通过自己跑代码测试总结以下几条，方便自己以后使用。
总的来说keras模型加载数据主要有三种方式：.fit(), .fit_generator()和.train_on_batch()。 1.fit(): 上函数，各个参数的意义就不解释了
fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)  从官方文档中可以看出，fit()是需要先把整个数据集加载进来，然后喂入网络，因为minist数据集比较小，这么做是可行的，但对于实际开发而言，这么做是不可行的，需要大量的内存资源，同时不能对数据进行在线提升。
一次性加载整个数据集的示例代码： 任务为猫和狗的二分类，train_data下包含cat和dog两个文件夹，代码将两个文件夹下图片和标签存入numpy数组，返回为训练数据和训练标签。
def load_data(): tran_imags = [] labels = [] seq_names = [&#39;cat&#39;,&#39;dog&#39;] for seq_name in seq_names: frames = sorted(os.listdir(os.path.join(root_path,&#39;data&#39;,&#39;train_data&#39;, seq_name))) for frame in frames: imgs = [os.path.join(root_path, &#39;data&#39;, &#39;train_data&#39;, seq_name, frame)] imgs = np.array(Image.open(imgs[0])) tran_imags.append(imgs) if seq_name==&#39;cat&#39;: labels.append(0) else: labels.append(1) return np.array(tran_imags), np.array(labels) ## train_data,train_labs = load_data() model.</description>
    </item>
    
    <item>
      <title>基于Keras图像相似度计算孪生网络</title>
      <link>https://yl305237731.github.io/post/keras_simi/</link>
      <pubDate>Mon, 12 Nov 2018 16:32:32 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/keras_simi/</guid>
      <description>import keras from keras.layers import Input,Dense,Conv2D from keras.layers import MaxPooling2D,Flatten,Convolution2D from keras.models import Model import os import numpy as np from PIL import Image from keras.optimizers import SGD from scipy import misc root_path = os.getcwd() train_names = [&#39;bear&#39;,&#39;blackswan&#39;,&#39;bus&#39;,&#39;camel&#39;,&#39;car&#39;,&#39;cows&#39;,&#39;dance&#39;,&#39;dog&#39;,&#39;hike&#39;,&#39;hoc&#39;,&#39;kite&#39;,&#39;lucia&#39;,&#39;mallerd&#39;,&#39;pigs&#39;,&#39;soapbox&#39;,&#39;stro&#39;,&#39;surf&#39;,&#39;swing&#39;,&#39;train&#39;,&#39;walking&#39;] test_names = [&#39;boat&#39;,&#39;dance-jump&#39;,&#39;drift-turn&#39;,&#39;elephant&#39;,&#39;libby&#39;] def load_data(seq_names,data_number,seq_len): #生成图片对 print(&#39;loading data.....&#39;) frame_num = 51 train_data1 = [] train_data2 = [] train_lab = [] count = 0 while count &amp;lt; data_number: count = count + 1 pos_neg = np.</description>
    </item>
    
    <item>
      <title>Keras 猫狗二分类</title>
      <link>https://yl305237731.github.io/post/kdog_cat/</link>
      <pubDate>Sun, 11 Nov 2018 16:36:27 +0800</pubDate>
      
      <guid>https://yl305237731.github.io/post/kdog_cat/</guid>
      <description>import keras from keras.models import Sequential from keras.layers import Dense,MaxPooling2D,Input,Flatten,Convolution2D,Dropout,GlobalAveragePooling2D from keras.optimizers import SGD from keras.callbacks import TensorBoard,ModelCheckpoint from PIL import Image import os import numpy as np from scipy import misc root_path = os.getcwd() def load_data(): tran_imags = [] labels = [] seq_names = [&#39;cat&#39;,&#39;dog&#39;] for seq_name in seq_names: frames = sorted(os.listdir(os.path.join(root_path,&#39;data&#39;,&#39;train_data&#39;, seq_name))) for frame in frames: imgs = [os.path.join(root_path, &#39;data&#39;, &#39;train_data&#39;, seq_name, frame)] imgs = np.array(Image.open(imgs[0])) tran_imags.append(imgs) if seq_name==&#39;cat&#39;: labels.</description>
    </item>
    
  </channel>
</rss>